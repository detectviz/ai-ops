# services/sre-assistant/src/sre_assistant/main.py
"""
SRE Assistant ä¸»ç¨‹å¼å…¥å£
æä¾› REST API ç«¯é»ä¾› Control Plane å‘¼å« (å·²é‡æ§‹ç‚ºéåŒæ­¥)
"""

from fastapi import FastAPI, HTTPException, Depends, Request, BackgroundTasks, Response
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from datetime import datetime, timezone, timedelta
import uuid
import asyncio
from typing import Dict, Any, Optional, List
import redis.asyncio as redis
import asyncpg
import time
import httpx
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST

# ä¾è³´é …å’Œèªè­‰é‚è¼¯å·²é‡æ§‹
from .dependencies import config_manager
from .auth import verify_token

from .contracts import (
    DiagnosticRequest,
    DiagnosticResponse,
    DiagnosticStatus,
    AlertAnalysisRequest,
    CapacityAnalysisRequest,
    CapacityAnalysisResponse,
    ExecuteRequest,
    DiagnosticHistoryList,
    DiagnosticHistoryItem,
    WorkflowTemplate,
    ToolStatus,
    Pagination,
)
from .workflow import SREWorkflow, SREWorkflowRequest

# --- çµæ§‹åŒ–æ—¥èªŒè¨­å®š ---
import structlog
import contextvars

# 1. ç‚ºè«‹æ±‚ ID å»ºç«‹ ContextVar
request_id_var = contextvars.ContextVar("request_id", default=None)

def configure_logging():
    """è¨­å®š structlog ä»¥è¼¸å‡º JSON æ ¼å¼çš„æ—¥èªŒ"""

    # structlog è™•ç†å™¨éˆ
    processors = [
        # å°‡ contextvars (å¦‚ request_id) åˆä½µåˆ°æ—¥èªŒè¨˜éŒ„ä¸­
        structlog.contextvars.merge_contextvars,
        # æ–°å¢æ—¥èªŒç´šåˆ¥å’Œåç¨±ç­‰æ¨™æº–å±¬æ€§
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        # æ–°å¢æ™‚é–“æˆ³
        structlog.processors.TimeStamper(fmt="iso"),
        # å¦‚æœæœ‰ç•°å¸¸ï¼Œæ ¼å¼åŒ–å®ƒ
        structlog.processors.format_exc_info,
        # æ¸²æŸ“ç‚º JSON
        structlog.processors.JSONRenderer(),
    ]

    structlog.configure(
        processors=processors,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

# åœ¨æ‡‰ç”¨å•Ÿå‹•æ™‚è¨­å®šæ—¥èªŒ
configure_logging()
logger = structlog.get_logger(__name__)
# --- çµæ§‹åŒ–æ—¥èªŒè¨­å®šçµæŸ ---

# å…¨åŸŸè®Šæ•¸
workflow: Optional[SREWorkflow] = None
redis_client: Optional[redis.Redis] = None
db_pool: Optional[asyncpg.Pool] = None
http_client: Optional[httpx.AsyncClient] = None
app_ready = False
startup_time = time.time() # æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚é–“

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    æ‡‰ç”¨ç¨‹å¼ç”Ÿå‘½é€±æœŸç®¡ç† (Application Lifespan Management)ã€‚

    é€™å€‹ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨æœƒåœ¨ FastAPI æ‡‰ç”¨å•Ÿå‹•æ™‚åŸ·è¡Œ `try` å€å¡Šä¸­çš„ç¨‹å¼ç¢¼ï¼Œ
    ä¸¦åœ¨æ‡‰ç”¨é—œé–‰æ™‚åŸ·è¡Œ `finally` å€å¡Šä¸­çš„ç¨‹å¼ç¢¼ã€‚
    é€™å°æ–¼åˆå§‹åŒ–å’Œæ¸…ç†è³‡æº (å¦‚è³‡æ–™åº«é€£æ¥ã€èƒŒæ™¯ä»»å‹™) éå¸¸æœ‰ç”¨ã€‚
    """
    global workflow, redis_client, db_pool, http_client, app_ready
    
    logger.info("ğŸš€ æ­£åœ¨å•Ÿå‹• SRE Assistant...")
    
    try:
        config = config_manager.get_config()

        # åˆå§‹åŒ– Redis Client
        redis_client = redis.from_url(
            config.redis.url,
            encoding="utf-8",
            decode_responses=True
        )
        await redis_client.ping()
        logger.info("âœ… å·²æˆåŠŸé€£æ¥åˆ° Redis")

        # åƒ…åœ¨éæ¸¬è©¦ç’°å¢ƒä¸­åˆå§‹åŒ– PostgreSQL é€£ç·šæ± 
        if config.get("environment") != "test":
            db_pool = await asyncpg.create_pool(
                dsn=config.database.url,
                min_size=5,
                max_size=20
            )
            # å˜—è©¦ç²å–ä¸€å€‹é€£ç·šä»¥é©—è­‰é€£ç·šèƒ½åŠ›
            async with db_pool.acquire():
                logger.info("âœ… å·²æˆåŠŸé€£æ¥åˆ° PostgreSQL")
        else:
            logger.info("ğŸ“ åµæ¸¬åˆ°æ¸¬è©¦ç’°å¢ƒï¼Œè·³é PostgreSQL é€£ç·šæ± åˆå§‹åŒ–ã€‚")

        # å»ºç«‹ä¸€å€‹å…±äº«çš„ã€å…·æœ‰é‡è©¦åŠŸèƒ½çš„ HTTP å®¢æˆ¶ç«¯
        # è¨­å®šé‡è©¦ç­–ç•¥ï¼šé‡å° 5xx éŒ¯èª¤å’Œé€£ç·šéŒ¯èª¤ï¼Œæœ€å¤šé‡è©¦ 3 æ¬¡
        transport = httpx.AsyncHTTPTransport(retries=3)
        # è¨­å®šé€£ç·šæ± é™åˆ¶
        limits = httpx.Limits(max_connections=100, max_keepalive_connections=20)
        http_client = httpx.AsyncClient(transport=transport, limits=limits, http2=True)
        logger.info("âœ… å…±äº«çš„ HTTP å®¢æˆ¶ç«¯å·²å»ºç«‹ï¼Œä¸¦è¨­å®šäº†é‡è©¦èˆ‡é€£ç·šæ± ")

        # åˆå§‹åŒ–å·¥ä½œæµç¨‹å¼•æ“ï¼Œä¸¦å‚³å…¥å…±äº«çš„å®¢æˆ¶ç«¯
        workflow = SREWorkflow(config, redis_client, http_client)

        app_ready = True
        logger.info("âœ… å·¥ä½œæµç¨‹å¼•æ“èˆ‡ä»»å‹™å„²å­˜å·²åˆå§‹åŒ–")

        logger.info("âœ… SRE Assistant å•Ÿå‹•å®Œæˆ")
        yield
    except Exception as e:
        logger.error(f"ğŸ’€ SRE Assistant å•Ÿå‹•å¤±æ•—: {e}", exc_info=True)
        app_ready = False
        yield # Still yield to allow the app to run and report not ready
    finally:
        # åœ¨æ‡‰ç”¨ç¨‹å¼é—œé–‰æ™‚ï¼Œå„ªé›…åœ°é—œé–‰æ‰€æœ‰å®¢æˆ¶ç«¯å’Œé€£ç·šæ± 
        if http_client:
            await http_client.aclose()
            logger.info("HTTP å®¢æˆ¶ç«¯å·²é—œé–‰")
        if db_pool:
            await db_pool.close()
            logger.info("PostgreSQL é€£ç·šæ± å·²é—œé–‰")
        if redis_client:
            await redis_client.close()
            logger.info("Redis é€£ç·šå·²é—œé–‰")
        logger.info("ğŸ›‘ æ­£åœ¨é—œé–‰ SRE Assistant...")
        app_ready = False


app = FastAPI(
    title="SRE Platform API",
    version="1.0.0",
    description="SRE Platform çš„éåŒæ­¥è¨ºæ–·èˆ‡åˆ†æå¼•æ“",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Request ID ä¸­ä»‹å±¤ ---
@app.middleware("http")
async def request_id_middleware(request: Request, call_next):
    """
    æ””æˆªè«‹æ±‚ï¼Œç”Ÿæˆå”¯ä¸€çš„ request_idï¼Œä¸¦å°‡å…¶æ”¾å…¥ context varã€‚
    """
    request_id = request.headers.get("X-Request-ID", str(uuid.uuid4()))
    request_id_var.set(request_id)
    response = await call_next(request)
    response.headers["X-Request-ID"] = request_id
    return response

# èªè­‰é‚è¼¯å·²é‡æ§‹è‡³ auth.py


# === èƒŒæ™¯ä»»å‹™åŸ·è¡Œå™¨ ===
async def run_workflow_bg(session_id: uuid.UUID, request: SREWorkflowRequest, request_type: str):
    """
    ä¸€å€‹é€šç”¨çš„åŒ…è£å‡½å¼ï¼Œç”¨æ–¼åœ¨èƒŒæ™¯åŸ·è¡Œå·¥ä½œæµç¨‹ä¸¦æ›´æ–°ä»»å‹™ç‹€æ…‹ã€‚

    Args:
        session_id: æ­¤æ¬¡ä»»å‹™çš„å”¯ä¸€æœƒè©± IDã€‚
        request: API è«‹æ±‚çš„è³‡æ–™æ¨¡å‹ã€‚
        request_type: è«‹æ±‚çš„é¡å‹ï¼Œç”¨æ–¼åœ¨å·¥ä½œæµç¨‹ä¸­é€²è¡Œè·¯ç”±ã€‚
    """
    initial_status = DiagnosticStatus(
        session_id=session_id,
        status="processing",
        progress=10,
        current_step=f"é–‹å§‹åŸ·è¡Œ {request_type} å·¥ä½œæµç¨‹"
    )
    # å°‡åˆå§‹ç‹€æ…‹å¯«å…¥ Redisï¼Œè¨­å®š 24 å°æ™‚éæœŸ
    await redis_client.set(
        str(session_id),
        initial_status.model_dump_json(),
        ex=86400
    )
    await workflow.execute(session_id, request, request_type)


# === API ç«¯é» ===

@app.get("/api/v1/healthz", tags=["Health"])
def check_liveness():
    """
    å¥åº·æª¢æŸ¥ç«¯é» - é©—è­‰æœå‹™æ˜¯å¦æ­£å¸¸é‹è¡Œ
    è¿”å› HealthStatus çµæ§‹ï¼Œç¬¦åˆ OpenAPI è¦ç¯„
    """
    from datetime import datetime

    # è¨ˆç®—é‹è¡Œæ™‚é–“ï¼ˆå¾æ‡‰ç”¨å•Ÿå‹•è‡³ä»Šï¼‰
    uptime = int(time.time() - startup_time)

    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "version": "1.0.0",
        "uptime": uptime
    }

@app.get("/api/v1/readyz", tags=["Health"])
async def check_readiness(response: Response):
    """
    åŸ·è¡Œæ·±å…¥çš„å°±ç·’æª¢æŸ¥ï¼Œé©—è­‰æ‰€æœ‰å¾Œç«¯ä¾è³´æ˜¯å¦å¯ç”¨ã€‚
    """
    checks = {
        "redis": False,
        "database": False,
        "prometheus": False,
        "loki": False,
        "control_plane": False
    }

    if not app_ready or not workflow or not redis_client:
        response.status_code = 503
        return {"ready": False, "checks": checks}

    async def check_db_health():
        """ä¸€å€‹ç°¡çŸ­çš„è¼”åŠ©å‡½å¼ï¼Œç”¨æ–¼æª¢æŸ¥è³‡æ–™åº«é€£ç·šã€‚"""
        if not db_pool:
            return True # åœ¨æ¸¬è©¦ç’°å¢ƒä¸­ï¼Œdb_pool ç‚º None
        try:
            async with db_pool.acquire() as connection:
                await connection.fetchval("SELECT 1")
            return True
        except Exception as e:
            logger.error(f"è³‡æ–™åº«å¥åº·æª¢æŸ¥å¤±æ•—: {e}")
            return False

    try:
        # ä½¿ç”¨ asyncio.gather ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰å¥åº·æª¢æŸ¥
        results = await asyncio.gather(
            redis_client.ping(),
            check_db_health(),
            workflow.prometheus_tool.check_health(),
            workflow.loki_tool.check_health(),
            workflow.control_plane_tool.check_health(),
            return_exceptions=True  # å³ä½¿æœ‰æª¢æŸ¥å¤±æ•—ä¹Ÿç¹¼çºŒ
        )

        # è™•ç†æª¢æŸ¥çµæœ
        checks["redis"] = results[0] is True
        checks["database"] = results[1] is True
        checks["prometheus"] = results[2] is True
        checks["loki"] = results[3] is True
        checks["control_plane"] = results[4] is True

        is_ready = all(checks.values())
        if not is_ready:
            response.status_code = 503

        return {"ready": is_ready, "checks": checks}
    except Exception as e:
        # é€™æ˜¯æœ€å¾Œçš„é˜²ç·šï¼Œæ•æ‰ asyncio.gather æœ¬èº«æˆ–å…¶ä»–æœªé æœŸçš„éŒ¯èª¤
        logger.error(f"å°±ç·’æª¢æŸ¥æ™‚ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}", exc_info=True)
        response.status_code = 503
        return {"ready": False, "checks": checks}

@app.get("/api/v1/metrics", tags=["Health"])
def get_metrics():
    """æä¾› Prometheus æ ¼å¼çš„æŒ‡æ¨™"""
    return Response(content=generate_latest(), media_type=CONTENT_TYPE_LATEST)

@app.post("/api/v1/diagnostics/deployment", tags=["Diagnostics"], status_code=202, response_model=DiagnosticResponse)
async def diagnose_deployment(
    request: DiagnosticRequest,
    background_tasks: BackgroundTasks,
    token: Dict[str, Any] = Depends(verify_token)
):
    """
    æ¥æ”¶éƒ¨ç½²è¨ºæ–·è«‹æ±‚ï¼Œä¸¦éåŒæ­¥è™•ç†ã€‚
    """
    session_id = uuid.uuid4()
    background_tasks.add_task(run_workflow_bg, session_id, request, "deployment")
    
    return DiagnosticResponse(
        session_id=session_id,
        status="accepted",
        message="éƒ¨ç½²è¨ºæ–·ä»»å‹™å·²æ¥å—ï¼Œæ­£åœ¨èƒŒæ™¯è™•ç†ä¸­ã€‚",
        estimated_time=120
    )

@app.get("/api/v1/diagnostics/{sessionId}/status", tags=["Diagnostics"], response_model=DiagnosticStatus)
async def get_diagnostic_status(
    sessionId: uuid.UUID,
    token: Dict[str, Any] = Depends(verify_token)
):
    """
    æŸ¥è©¢éåŒæ­¥è¨ºæ–·ä»»å‹™çš„ç‹€æ…‹ã€‚
    """
    task_json = await redis_client.get(str(sessionId))
    if not task_json:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æŒ‡å®šçš„è¨ºæ–·ä»»å‹™")

    return DiagnosticStatus.model_validate_json(task_json)

# --- New Endpoints ---

@app.post("/api/v1/diagnostics/alerts", tags=["Diagnostics"], status_code=202, response_model=DiagnosticResponse)
async def analyze_alerts(
    request: AlertAnalysisRequest,
    background_tasks: BackgroundTasks,
    token: Dict[str, Any] = Depends(verify_token)
):
    """
    æ¥æ”¶å‘Šè­¦åˆ†æè«‹æ±‚ï¼Œä¸¦éåŒæ­¥è™•ç†ã€‚
    """
    session_id = uuid.uuid4()
    background_tasks.add_task(run_workflow_bg, session_id, request, "alert_analysis")

    return DiagnosticResponse(
        session_id=session_id,
        status="accepted",
        message="å‘Šè­¦åˆ†æä»»å‹™å·²æ¥å—ï¼Œæ­£åœ¨èƒŒæ™¯è™•ç†ä¸­ã€‚",
        estimated_time=60
    )

@app.post("/api/v1/diagnostics/capacity", tags=["Diagnostics"], status_code=200, response_model=CapacityAnalysisResponse)
async def analyze_capacity(
    request: CapacityAnalysisRequest,
    token: Dict[str, Any] = Depends(verify_token)
):
    """
    æ¥æ”¶å®¹é‡åˆ†æè«‹æ±‚ï¼Œä¸¦åŒæ­¥è™•ç†ã€‚
    """
    if not workflow:
        raise HTTPException(status_code=503, detail="å·¥ä½œæµç¨‹å¼•æ“å°šæœªåˆå§‹åŒ–ã€‚")

    # å‘¼å«å·¥ä½œæµç¨‹ä¸­æ–°çš„ã€å…·å‚™çœŸå¯¦é‚è¼¯çš„æ–¹æ³•
    return await workflow.analyze_capacity(request)

@app.post("/api/v1/execute", tags=["Diagnostics"], status_code=202, response_model=DiagnosticResponse)
async def execute_query(
    request: ExecuteRequest,
    background_tasks: BackgroundTasks,
    token: Dict[str, Any] = Depends(verify_token)
):
    """
    æ¥æ”¶è‡ªç„¶èªè¨€æŸ¥è©¢è«‹æ±‚ï¼Œä¸¦éåŒæ­¥è™•ç†ã€‚
    """
    session_id = uuid.uuid4()
    background_tasks.add_task(run_workflow_bg, session_id, request, "execute_query")

    return DiagnosticResponse(
        session_id=session_id,
        status="accepted",
        message="è‡ªç„¶èªè¨€æŸ¥è©¢ä»»å‹™å·²æ¥å—ï¼Œæ­£åœ¨èƒŒæ™¯è™•ç†ä¸­ã€‚",
        estimated_time=180
    )

@app.get("/api/v1/diagnostics/history", tags=["Diagnostics"], response_model=DiagnosticHistoryList)
async def get_diagnostic_history(
    page: int = 1,
    page_size: int = 20,
    token: Dict[str, Any] = Depends(verify_token)
):
    """
    æŸ¥è©¢æ­·å²è¨ºæ–·è¨˜éŒ„ (éª¨æ¶)ã€‚
    """
    # æ¨¡æ“¬æ•¸æ“š
    items = [
        DiagnosticHistoryItem(
            session_id=uuid.uuid4(),
            incident_id="INC-2025-001",
            status="completed",
            created_at=datetime.now(timezone.utc) - timedelta(hours=1),
            completed_at=datetime.now(timezone.utc) - timedelta(minutes=50),
            summary="åˆæ­¥è¨ºæ–·æœªç™¼ç¾æ˜é¡¯å•é¡Œã€‚"
        )
    ]
    return DiagnosticHistoryList(
        items=items,
        pagination=Pagination(page=page, page_size=page_size, total=len(items), total_pages=1)
    )

@app.get("/api/v1/workflows/templates", tags=["Workflows"], response_model=List[WorkflowTemplate])
async def list_workflow_templates(token: Dict[str, Any] = Depends(verify_token)):
    """
    ç²å–å·¥ä½œæµæ¨¡æ¿ (éª¨æ¶)ã€‚
    """
    # æ¨¡æ“¬æ•¸æ“š
    templates = [
        WorkflowTemplate(id="deployment_diagnosis", name="éƒ¨ç½²å•é¡Œè¨ºæ–·", description="åˆ†æéƒ¨ç½²å¤±æ•—æˆ–æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•ç•°å¸¸å•é¡Œã€‚", parameters=[]),
        WorkflowTemplate(id="alert_correlation", name="å‘Šè­¦é—œè¯åˆ†æ", description="åˆ†æå¤šå€‹å‘Šè­¦ä¹‹é–“çš„é—œè¯æ€§ï¼Œå°‹æ‰¾æ ¹æœ¬åŸå› ã€‚", parameters=[{"name": "alert_ids", "type": "list"}]),
    ]
    return templates

@app.get("/api/v1/tools/status", tags=["Tools"], response_model=Dict[str, ToolStatus])
async def get_tools_status(token: Dict[str, Any] = Depends(verify_token)):
    """
    ç²å–å·¥å…·ç‹€æ…‹ (éª¨æ¶)ã€‚
    """
    # æ¨¡æ“¬æ•¸æ“š
    now = datetime.now(timezone.utc)
    return {
        "prometheus": ToolStatus(status="healthy", last_checked=now),
        "loki": ToolStatus(status="healthy", last_checked=now),
        "control_plane": ToolStatus(status="unhealthy", last_checked=now, details={"error": "Connection timeout"})
    }
