# services/sre-assistant/src/sre_assistant/workflow.py
"""
SRE å·¥ä½œæµç¨‹å”èª¿å™¨ (å·²é‡æ§‹ä»¥æ”¯æ´éåŒæ­¥ä»»å‹™)
"""

import asyncio
import functools
import structlog
import uuid
import httpx
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone, timedelta

from typing import Union
from .contracts import (
    DiagnosticRequest,
    DiagnosticResult,
    DiagnosticStatus,
    ToolResult,
    ToolError,
    Finding,
    AlertAnalysisRequest,
    CapacityAnalysisRequest,
    ExecuteRequest,
    CapacityAnalysisResponse,
)

from .tools.prometheus_tool import PrometheusQueryTool
from .tools.loki_tool import LokiLogQueryTool
from .tools.control_plane_tool import ControlPlaneTool

# Define a union type for all possible request models
SREWorkflowRequest = Union[DiagnosticRequest, AlertAnalysisRequest, CapacityAnalysisRequest, ExecuteRequest]

logger = structlog.get_logger(__name__)


class SREWorkflow:
    """
    ä¸»è¦çš„ SRE å·¥ä½œæµç¨‹å”èª¿å™¨
    
    è² è²¬ï¼š
    1. æ¥æ”¶è¨ºæ–·è«‹æ±‚
    2. ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹è¨ºæ–·å·¥å…·
    3. æ•´åˆåˆ†æçµæœ
    4. æ›´æ–°å…±äº«çš„ä»»å‹™ç‹€æ…‹å­—å…¸
    """
    
    def __init__(self, config, redis_client, http_client: httpx.AsyncClient):
        """åˆå§‹åŒ–å·¥ä½œæµç¨‹"""
        self.config = config
        self.redis_client = redis_client
        self.http_client = http_client # å„²å­˜å…±äº«çš„ HTTP å®¢æˆ¶ç«¯

        # å°‡å…±äº«çš„å®¢æˆ¶ç«¯å’Œ redis_client å‚³éçµ¦å·¥å…·
        self.prometheus_tool = PrometheusQueryTool(config, self.http_client, self.redis_client)
        self.loki_tool = LokiLogQueryTool(config, self.http_client)
        self.control_plane_tool = ControlPlaneTool(config, self.http_client)
        self.parallel_diagnosis = config.workflow.get("parallel_diagnosis", True)
        self.diagnosis_timeout = config.workflow.get("diagnosis_timeout_seconds", 120)
        self.max_retries = config.workflow.get("max_retries", 2)  # 2 retries = 3 total attempts
        self.retry_delay = config.workflow.get("retry_delay_seconds", 1)
        logger.info("âœ… SRE å·¥ä½œæµç¨‹åˆå§‹åŒ–å®Œæˆ")
    
    async def _get_task_status(self, session_id: uuid.UUID) -> Optional[DiagnosticStatus]:
        """Helper to get task status from Redis."""
        task_json = await self.redis_client.get(str(session_id))
        if task_json:
            return DiagnosticStatus.model_validate_json(task_json)
        return None

    async def _update_task_status(self, session_id: uuid.UUID, status: DiagnosticStatus):
        """Helper to update task status in Redis."""
        await self.redis_client.set(str(session_id), status.model_dump_json(), ex=86400)

    async def execute(self, session_id: uuid.UUID, request: SREWorkflowRequest, request_type: str):
        """
        åŸ·è¡Œä¸»è¦å·¥ä½œæµç¨‹ (èƒŒæ™¯ä»»å‹™)ã€‚
        """
        start_time = datetime.now(timezone.utc)
        logger.info(f"ğŸš€ [Session: {session_id}] é–‹å§‹åŸ·è¡Œ {request_type} å·¥ä½œæµç¨‹...")
        
        try:
            status = await self._get_task_status(session_id)
            if not status:
                logger.error(f"ç„¡æ³•å¾ Redis ä¸­æ‰¾åˆ°ä»»å‹™ç‹€æ…‹: {session_id}")
                return

            result_data: Optional[Union[DiagnosticResult, CapacityAnalysisResponse]] = None
            if request_type == "deployment" and isinstance(request, DiagnosticRequest):
                result_data = await self._diagnose_deployment(session_id, request, status)
            elif request_type == "alert_analysis" and isinstance(request, AlertAnalysisRequest):
                result_data = await self._diagnose_alerts(session_id, request, status)
            elif request_type == "execute_query" and isinstance(request, ExecuteRequest):
                result_data = await self._execute_query(session_id, request, status)
            elif request_type == "capacity_analysis" and isinstance(request, CapacityAnalysisRequest):
                result_data = await self._analyze_capacity(session_id, request, status)
            else:
                raise ValueError(f"æœªçŸ¥çš„è«‹æ±‚é¡å‹æˆ–è«‹æ±‚èˆ‡é¡å‹ä¸åŒ¹é…: {request_type}")

            execution_time = (datetime.now(timezone.utc) - start_time).total_seconds()
            
            final_status = await self._get_task_status(session_id)
            if final_status:
                if isinstance(result_data, DiagnosticResult):
                    final_status.result = result_data
                    final_status.result.execution_time = execution_time
                # Note: CapacityAnalysisResponse is a direct return, not part of DiagnosticStatus
                final_status.status = "completed"
                final_status.progress = 100
                final_status.current_step = "è¨ºæ–·å®Œæˆ"
                await self._update_task_status(session_id, final_status)
            
            logger.info(f"âœ… [Session: {session_id}] å·¥ä½œæµç¨‹å®Œæˆ (è€—æ™‚ {execution_time:.2f}s)")
            
        except Exception as e:
            logger.error(f"âŒ [Session: {session_id}] å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
            final_status = await self._get_task_status(session_id)
            if final_status:
                final_status.status = "failed"
                final_status.error = f"å·¥ä½œæµç¨‹ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}"
                await self._update_task_status(session_id, final_status)
    
    async def _diagnose_deployment(self, session_id: uuid.UUID, request: DiagnosticRequest, status: DiagnosticStatus) -> DiagnosticResult:
        """
        è¨ºæ–·éƒ¨ç½²å•é¡Œ
        """
        logger.info(f"ğŸ” [Session: {session_id}] è¨ºæ–·éƒ¨ç½²: {request.affected_services[0]}")
        
        status.current_step = "æº–å‚™è¨ºæ–·ä»»å‹™"
        status.progress = 20
        await self._update_task_status(session_id, status)
        
        tool_tasks = [
            ("prometheus", functools.partial(self.prometheus_tool.execute, {"service": request.affected_services[0]})),
            ("loki", functools.partial(self.loki_tool.execute, {"service": request.affected_services[0]})),
            ("audit", functools.partial(self.control_plane_tool.query_audit_logs, {"resource_type": "deployment", "search": request.affected_services[0]})),
            ("incidents", functools.partial(self.control_plane_tool.query_incidents, {"search": request.affected_services[0], "status": "new,acknowledged"}))
        ]
        
        status.current_step = "ä¸¦è¡ŒåŸ·è¡Œè¨ºæ–·å·¥å…· (å«é‡è©¦)"
        status.progress = 50
        await self._update_task_status(session_id, status)

        results = await self._execute_parallel_tasks(tool_tasks)
        
        status.current_step = "åˆ†æè¨ºæ–·çµæœ"
        status.progress = 80
        await self._update_task_status(session_id, status)
        
        return self._analyze_deployment_results(results)

    async def _run_task_with_retry(self, name: str, coro_factory) -> Any:
        """
        åŸ·è¡Œå–®ä¸€ä»»å‹™ï¼Œä¸¦å¸¶æœ‰é‡è©¦å’ŒæŒ‡æ•¸é€€é¿é‚è¼¯ã€‚
        """
        last_exception = None
        for attempt in range(self.max_retries + 1):
            try:
                task_coro = coro_factory()
                result = await asyncio.wait_for(task_coro, timeout=self.diagnosis_timeout)
                if attempt > 0:
                    logger.info(f"âœ… å·¥å…· {name} åœ¨ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¾ŒæˆåŠŸã€‚")
                return result
            except Exception as e:
                last_exception = e
                if attempt < self.max_retries:
                    delay = self.retry_delay * (2 ** attempt)
                    logger.warning(
                        f"âš ï¸ å·¥å…· {name} ç¬¬ {attempt + 1}/{self.max_retries + 1} æ¬¡åŸ·è¡Œå¤±æ•—: {e}. "
                        f"å°‡åœ¨ {delay:.2f} ç§’å¾Œé‡è©¦..."
                    )
                    await asyncio.sleep(delay)
                else:
                    logger.error(
                        f"âŒ å·¥å…· {name} åœ¨ {self.max_retries + 1} æ¬¡å˜—è©¦å¾Œæœ€çµ‚å¤±æ•—ã€‚"
                    )
                    raise last_exception

    async def _execute_parallel_tasks(self, tool_tasks: List[tuple]) -> Dict[str, ToolResult]:
        """
        ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹ç•°æ­¥è¨ºæ–·å·¥å…·ä»»å‹™ã€‚
        """
        results = {}

        tasks_to_gather = [
            self._run_task_with_retry(name, coro_factory) for name, coro_factory in tool_tasks
        ]

        task_results = await asyncio.gather(*tasks_to_gather, return_exceptions=True)
        
        for i, (name, _) in enumerate(tool_tasks):
            result = task_results[i]
            if isinstance(result, Exception):
                error_code = "TOOL_TIMEOUT_ERROR" if isinstance(result, asyncio.TimeoutError) else "TOOL_EXECUTION_ERROR"
                results[name] = ToolResult(success=False, error=ToolError(code=error_code, message=str(result)))
            else:
                results[name] = result
        return results
    
    def _analyze_deployment_results(self, results: Dict[str, ToolResult]) -> DiagnosticResult:
        """åˆ†æéƒ¨ç½²è¨ºæ–·çµæœ"""
        all_findings = []
        tools_used = []
        
        if "prometheus" in results and results["prometheus"].success:
            tools_used.append("PrometheusQueryTool")
            metrics = results["prometheus"].data
            if metrics and float(metrics.get("cpu_usage", "0%").replace("%", "")) > 80:
                all_findings.append(Finding(source="Prometheus", severity="critical", message="CPU ä½¿ç”¨ç‡éé«˜", evidence=metrics))
            if metrics and float(metrics.get("memory_usage", "0%").replace("%", "")) > 90:
                all_findings.append(Finding(source="Prometheus", severity="critical", message="è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜", evidence=metrics))

        if "loki" in results and results["loki"].success:
            tools_used.append("LokiLogQueryTool")
            logs = results["loki"].data
            analysis = logs.get("analysis", {})
            critical_indicators = analysis.get("critical_indicators")
            if critical_indicators:
                all_findings.append(Finding(source="Loki", severity="critical", message=f"ç™¼ç¾åš´é‡æ—¥èªŒæŒ‡æ¨™: {', '.join(critical_indicators)}", evidence=logs))

        if "audit" in results and results["audit"].success:
            tools_used.append("ControlPlaneTool (Audit)")
            audit = results["audit"].data
            if audit.get("logs"):
                all_findings.append(Finding(source="Control-Plane", severity="info", message=f"ç™¼ç¾ {len(audit['logs'])} ç­†ç›¸é—œå¯©è¨ˆæ—¥èªŒã€‚", evidence=audit))

        if "incidents" in results and results["incidents"].success:
            tools_used.append("ControlPlaneTool (Incidents)")
            incidents = results["incidents"].data
            if incidents.get("incidents"):
                all_findings.append(Finding(source="Control-Plane", severity="warning", message=f"ç™¼ç¾ {len(incidents['incidents'])} ä»¶ç›¸é—œçš„æ´»èºäº‹ä»¶ã€‚", evidence=incidents))

        if all_findings:
            summary = f"è¨ºæ–·å®Œæˆï¼Œå…±ç™¼ç¾ {len(all_findings)} å€‹å•é¡Œé»ã€‚"
            recommended_actions = ["è«‹æ ¹æ“šç™¼ç¾çš„è©³ç´°è³‡è¨Šé€²è¡Œæ·±å…¥èª¿æŸ¥ã€‚"]
            confidence_score = 0.8
        else:
            summary = "åˆæ­¥è¨ºæ–·æœªç™¼ç¾æ˜é¡¯å•é¡Œã€‚"
            recommended_actions = ["å»ºè­°æ‰‹å‹•æª¢æŸ¥æœå‹™æ—¥èªŒå’Œç›£æ§å„€è¡¨æ¿ã€‚"]
            confidence_score = 0.5
            
        return DiagnosticResult(
            summary=summary,
            findings=all_findings,
            recommended_actions=recommended_actions,
            confidence_score=confidence_score,
            tools_used=tools_used
        )

    async def _analyze_capacity(self, session_id: uuid.UUID, request: CapacityAnalysisRequest, status: DiagnosticStatus) -> CapacityAnalysisResponse:
        """
        åˆ†ææŒ‡å®šè³‡æºçš„å®¹é‡ä½¿ç”¨æƒ…æ³å’Œè¶¨å‹¢ã€‚
        """
        logger.info(f"ğŸ“ˆ [Session: {session_id}] é–‹å§‹åˆ†æå®¹é‡: {request.resource_ids}")

        if not request.resource_ids:
            # é›–ç„¶ API å¥‘ç´„è¦æ±‚è‡³å°‘ä¸€å€‹ IDï¼Œä½†é‚„æ˜¯åšå€‹é˜²ç¦¦æ€§æª¢æŸ¥
            raise ValueError("CapacityAnalysisRequest ä¸­å¿…é ˆè‡³å°‘æä¾›ä¸€å€‹ resource_idã€‚")

        # æš«æ™‚åªè™•ç†ç¬¬ä¸€å€‹ resource_id
        resource_id = request.resource_ids[0]

        # 1. å¾ Control Plane ç²å–è³‡æºè©³æƒ…ï¼Œä»¥å¾—åˆ°æœå‹™åç¨±
        resource_details_result = await self.control_plane_tool.get_resource_details(resource_id)
        if not resource_details_result.success:
            logger.error(f"ç„¡æ³•ç²å–è³‡æºè©³æƒ… {resource_id}: {resource_details_result.error.message}")
            # é€™è£¡å¯ä»¥æ ¹æ“šéŒ¯èª¤é¡å‹æ±ºå®šæ˜¯å¦è¦æ‹‹å‡ºç•°å¸¸æˆ–å›å‚³ä¸€å€‹éŒ¯èª¤çš„å›æ‡‰
            # ç‚ºäº†ç°¡å–®èµ·è¦‹ï¼Œæˆ‘å€‘ç›´æ¥æ‹‹å‡ºç•°å¸¸ï¼Œè®“å¤–å±¤çš„ try-except æ•æ‰
            raise Exception(f"ç²å–è³‡æº {resource_id} è©³æƒ…å¤±æ•—ã€‚")

        service_name = resource_details_result.data.get("name")
        if not service_name:
            raise ValueError(f"è³‡æº {resource_id} çš„è©³ç´°è³‡æ–™ä¸­ç¼ºå°‘ 'name' æ¬„ä½ã€‚")

        # 2. ä½¿ç”¨æœå‹™åç¨±å¾ Prometheus æŸ¥è©¢é£½å’Œåº¦æŒ‡æ¨™
        prometheus_params = {"service": service_name, "metric_type": "saturation"}
        saturation_result = await self.prometheus_tool.execute(prometheus_params)
        if not saturation_result.success:
            logger.error(f"ç„¡æ³•ç²å–æœå‹™ {service_name} çš„é£½å’Œåº¦æŒ‡æ¨™: {saturation_result.error.message}")
            raise Exception(f"ç²å–æœå‹™ {service_name} çš„é£½å’Œåº¦æŒ‡æ¨™å¤±æ•—ã€‚")

        metrics = saturation_result.data

        # 3. é€²è¡Œç°¡å–®çš„åˆ†æå’Œé æ¸¬
        # æ³¨æ„ï¼šé€™æ˜¯ä¸€å€‹éå¸¸ç°¡åŒ–çš„æ¨¡å‹ï¼ŒçœŸå¯¦ä¸–ç•Œä¸­æœƒä½¿ç”¨æ›´è¤‡é›œçš„æ™‚é–“åºåˆ—é æ¸¬æ¼”ç®—æ³•
        cpu_usage = float(metrics.get("cpu_usage", "0%").strip('%'))
        mem_usage = float(metrics.get("memory_usage", "0%").strip('%'))

        recommendations = []
        if cpu_usage > 85.0:
            recommendations.append({
                "type": "scale_up",
                "resource": resource_id,
                "priority": "high",
                "reasoning": f"ç•¶å‰ CPU ä½¿ç”¨ç‡ ({cpu_usage:.1f}%) å·²è¶…é 85% çš„é–¾å€¼ã€‚"
            })
        if mem_usage > 85.0:
            recommendations.append({
                "type": "scale_up",
                "resource": resource_id,
                "priority": "high",
                "reasoning": f"ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨ç‡ ({mem_usage:.1f}%) å·²è¶…é 85% çš„é–¾å€¼ã€‚"
            })

        # 4. å»ºç«‹ä¸¦å›å‚³å›æ‡‰
        return CapacityAnalysisResponse(
            current_usage={"peak": cpu_usage, "average": mem_usage},
            forecast={"trend": "stable", "days_to_capacity": 90}, # é æ¸¬éƒ¨åˆ†æš«æ™‚ä½¿ç”¨å‡è³‡æ–™
            recommendations=recommendations
        )

    async def _diagnose_alerts(self, session_id: uuid.UUID, request: AlertAnalysisRequest, status: DiagnosticStatus) -> DiagnosticResult:
        """
        è¨ºæ–·å‘Šè­¦å•é¡Œ
        """
        logger.info(f"ğŸ” [Session: {session_id}] é–‹å§‹è¨ºæ–·å‘Šè­¦: {request.alert_ids}")
        return DiagnosticResult(summary="å‘Šè­¦åˆ†æåŠŸèƒ½å°šæœªå®Œå…¨å¯¦ä½œã€‚", findings=[], recommended_actions=[])

    async def _execute_query(self, session_id: uuid.UUID, request: ExecuteRequest, status: DiagnosticStatus) -> DiagnosticResult:
        """
        åŸ·è¡Œè‡ªç„¶èªè¨€æŸ¥è©¢ (éª¨æ¶)
        """
        logger.info(f"ğŸ¤– [Session: {session_id}] åŸ·è¡ŒæŸ¥è©¢: {request.query}")
        return DiagnosticResult(summary=f"å·²åŸ·è¡ŒæŸ¥è©¢: '{request.query}'", findings=[], recommended_actions=["ç„¡"])
