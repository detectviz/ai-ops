# services/sre-assistant/src/sre_assistant/workflow.py
"""
SRE å·¥ä½œæµç¨‹å”èª¿å™¨ (å·²é‡æ§‹ä»¥æ”¯æ´éåŒæ­¥ä»»å‹™)
"""

import asyncio
import functools
import structlog
import uuid
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone, timedelta

from typing import Union
from .contracts import (
    DiagnosticRequest,
    DiagnosticResult,
    DiagnosticStatus,
    ToolResult,
    ToolError,
    Finding,
    AlertAnalysisRequest,
    CapacityAnalysisRequest,
    ExecuteRequest,
    CapacityAnalysisResponse,
)

from .tools.prometheus_tool import PrometheusQueryTool
from .tools.loki_tool import LokiLogQueryTool
from .tools.control_plane_tool import ControlPlaneTool

# Define a union type for all possible request models
SREWorkflowRequest = Union[DiagnosticRequest, AlertAnalysisRequest, CapacityAnalysisRequest, ExecuteRequest]

logger = structlog.get_logger(__name__)


class SREWorkflow:
    """
    ä¸»è¦çš„ SRE å·¥ä½œæµç¨‹å”èª¿å™¨
    
    è² è²¬ï¼š
    1. æ¥æ”¶è¨ºæ–·è«‹æ±‚
    2. ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹è¨ºæ–·å·¥å…·
    3. æ•´åˆåˆ†æçµæœ
    4. æ›´æ–°å…±äº«çš„ä»»å‹™ç‹€æ…‹å­—å…¸
    """
    
    def __init__(self, config, redis_client, http_client):
        """
        åˆå§‹åŒ–å·¥ä½œæµç¨‹

        Args:
            config: æ‡‰ç”¨ç¨‹å¼è¨­å®šç‰©ä»¶ã€‚
            redis_client: éåŒæ­¥ Redis å®¢æˆ¶ç«¯ã€‚
            http_client: å…±äº«çš„ httpx.AsyncClient å¯¦ä¾‹ã€‚
        """
        self.config = config
        self.redis_client = redis_client
        self.http_client = http_client # å„²å­˜å…±äº«å®¢æˆ¶ç«¯

        # å°‡å…±äº«çš„ http_client å’Œå…¶ä»–ä¾è³´æ³¨å…¥åˆ°æ‰€æœ‰å·¥å…·ä¸­
        self.prometheus_tool = PrometheusQueryTool(config, http_client, self.redis_client)
        self.loki_tool = LokiLogQueryTool(config, http_client)
        self.control_plane_tool = ControlPlaneTool(config, http_client)

        self.parallel_diagnosis = config.workflow.get("parallel_diagnosis", True)
        self.diagnosis_timeout = config.workflow.get("diagnosis_timeout_seconds", 120)
        self.max_retries = config.workflow.get("max_retries", 2)
        self.retry_delay = config.workflow.get("retry_delay_seconds", 1)
        logger.info("âœ… SRE å·¥ä½œæµç¨‹åˆå§‹åŒ–å®Œæˆ (ä½¿ç”¨å…±äº« HTTP å®¢æˆ¶ç«¯)")
    
    async def _get_task_status(self, session_id: uuid.UUID) -> Optional[DiagnosticStatus]:
        """Helper to get task status from Redis."""
        task_json = await self.redis_client.get(str(session_id))
        if task_json:
            return DiagnosticStatus.model_validate_json(task_json)
        return None

    async def _update_task_status(self, session_id: uuid.UUID, status: DiagnosticStatus):
        """Helper to update task status in Redis."""
        await self.redis_client.set(str(session_id), status.model_dump_json(), ex=86400)

    async def execute(self, session_id: uuid.UUID, request: SREWorkflowRequest, request_type: str):
        """
        åŸ·è¡Œä¸»è¦å·¥ä½œæµç¨‹ (èƒŒæ™¯ä»»å‹™)ã€‚

        é€™æ˜¯å·¥ä½œæµç¨‹çš„æ ¸å¿ƒå…¥å£é»ï¼Œç”±èƒŒæ™¯ä»»å‹™å‘¼å«ã€‚å®ƒæœƒæ ¹æ“š `request_type`
        å°‡è«‹æ±‚è·¯ç”±åˆ°å°æ‡‰çš„è™•ç†å‡½å¼ (ä¾‹å¦‚ `_diagnose_deployment`)ã€‚

        å®ƒé‚„è² è²¬ï¼š
        - è¨˜éŒ„åŸ·è¡Œæ™‚é–“ã€‚
        - å¾ Redis ç²å–å’Œæ›´æ–°ä»»å‹™ç‹€æ…‹ã€‚
        - æ•ç²å’Œè¨˜éŒ„ä»»ä½•æœªé æœŸçš„éŒ¯èª¤ã€‚
        
        Args:
            session_id: æ­¤ä»»å‹™çš„å”¯ä¸€æœƒè©± IDã€‚
            request: SRE è«‹æ±‚ç‰©ä»¶ (ä¸€å€‹ Pydantic æ¨¡å‹)ã€‚
            request_type: è«‹æ±‚çš„é¡å‹ï¼Œç”¨æ–¼æ±ºå®šåŸ·è¡Œå“ªå€‹å­æµç¨‹ã€‚
        """
        start_time = datetime.now(timezone.utc)
        logger.info(f"ğŸš€ [Session: {session_id}] é–‹å§‹åŸ·è¡Œ {request_type} å·¥ä½œæµç¨‹...")
        
        try:
            status = await self._get_task_status(session_id)
            if not status:
                logger.error(f"ç„¡æ³•å¾ Redis ä¸­æ‰¾åˆ°ä»»å‹™ç‹€æ…‹: {session_id}")
                return

            result_data = None
            if request_type == "deployment" and isinstance(request, DiagnosticRequest):
                result_data = await self._diagnose_deployment(session_id, request, status)
            elif request_type == "alert_analysis" and isinstance(request, AlertAnalysisRequest):
                result_data = await self._diagnose_alerts(session_id, request, status)
            elif request_type == "execute_query" and isinstance(request, ExecuteRequest):
                result_data = await self._execute_query(session_id, request, status)
            elif request_type == "capacity_analysis" and isinstance(request, CapacityAnalysisRequest):
                result_data = await self._analyze_capacity(session_id, request, status)
            else:
                raise ValueError(f"æœªçŸ¥çš„è«‹æ±‚é¡å‹æˆ–è«‹æ±‚èˆ‡é¡å‹ä¸åŒ¹é…: {request_type}")

            execution_time = (datetime.now(timezone.utc) - start_time).total_seconds()
            
            final_status = await self._get_task_status(session_id)
            if final_status and isinstance(result_data, DiagnosticResult):
                final_status.result = result_data
                final_status.result.execution_time = execution_time
                final_status.status = "completed"
                final_status.progress = 100
                final_status.current_step = "è¨ºæ–·å®Œæˆ"
                await self._update_task_status(session_id, final_status)
            
            logger.info(f"âœ… [Session: {session_id}] å·¥ä½œæµç¨‹å®Œæˆ (è€—æ™‚ {execution_time:.2f}s)")
            
        except Exception as e:
            logger.error(f"âŒ [Session: {session_id}] å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
            final_status = await self._get_task_status(session_id)
            if final_status:
                final_status.status = "failed"
                final_status.error = f"å·¥ä½œæµç¨‹ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}"
                await self._update_task_status(session_id, final_status)
    
    async def _diagnose_deployment(self, session_id: uuid.UUID, request: DiagnosticRequest, status: DiagnosticStatus) -> DiagnosticResult:
        """
        è¨ºæ–·éƒ¨ç½²å•é¡Œ
        """
        logger.info(f"ğŸ” [Session: {session_id}] è¨ºæ–·éƒ¨ç½²: {request.affected_services[0]}")
        
        status.current_step = "æº–å‚™è¨ºæ–·ä»»å‹™"
        status.progress = 20
        await self._update_task_status(session_id, status)
        
        # ä½¿ç”¨ functools.partial ä¾†å‰µå»ºå¯é‡è¤‡å‘¼å«çš„ä»»å‹™å·¥å» 
        # å®šç¾©è¦ä¸¦è¡ŒåŸ·è¡Œçš„å·¥å…·ä»»å‹™
        # æ¯å€‹ä»»å‹™éƒ½æ˜¯ä¸€å€‹å…ƒçµ„ (åç¨±, å¯å‘¼å«çš„å”ç¨‹å·¥å» )
        # functools.partial ç”¨æ–¼é å…ˆç¶å®šåƒæ•¸ï¼Œä½¿æ‰€æœ‰å·¥å…·çš„å‘¼å«ç°½åä¸€è‡´
        # å®šç¾©è¦ä¸¦è¡ŒåŸ·è¡Œçš„å·¥å…·ä»»å‹™
        # æ¯å€‹ä»»å‹™éƒ½æ˜¯ä¸€å€‹å…ƒçµ„ (åç¨±, å¯å‘¼å«çš„å”ç¨‹å·¥å» )
        # functools.partial ç”¨æ–¼é å…ˆç¶å®šåƒæ•¸ï¼Œä½¿æ‰€æœ‰å·¥å…·çš„å‘¼å«ç°½åä¸€è‡´
        tool_tasks = [
            ("prometheus", functools.partial(self.prometheus_tool.execute, {"service": request.affected_services[0]})),
            ("loki", functools.partial(self.loki_tool.execute, {"service": request.affected_services[0]})),
            ("audit", functools.partial(self.control_plane_tool.query_audit_logs, {"resource_type": "deployment", "search": request.affected_services[0]})),
            # æ–°å¢ï¼šæ ¹æ“šæ¸¬è©¦è¦æ±‚ï¼ŒåŠ å…¥å°ç›¸é—œäº‹ä»¶çš„æŸ¥è©¢
            ("incidents", functools.partial(self.control_plane_tool.query_incidents, {"search": request.affected_services[0], "status": "new,acknowledged"}))
        ]
        
        status.current_step = "ä¸¦è¡ŒåŸ·è¡Œè¨ºæ–·å·¥å…· (å«é‡è©¦)"
        status.progress = 50
        await self._update_task_status(session_id, status)

        results = await self._execute_parallel_tasks(tool_tasks)
        
        status.current_step = "åˆ†æè¨ºæ–·çµæœ"
        status.progress = 80
        await self._update_task_status(session_id, status)
        
        return self._analyze_deployment_results(results, request)

    async def _run_task_with_retry(self, name: str, coro_factory) -> Any:
        """
        åŸ·è¡Œå–®ä¸€ä»»å‹™ï¼Œä¸¦å¸¶æœ‰é‡è©¦å’ŒæŒ‡æ•¸é€€é¿é‚è¼¯ã€‚
        
        Args:
            name: å·¥å…·çš„åç¨± (ç”¨æ–¼æ—¥èªŒ)ã€‚
            coro_factory: ä¸€å€‹ç„¡åƒæ•¸çš„å‡½æ•¸ï¼Œæ¯æ¬¡å‘¼å«éƒ½æœƒè¿”å›ä¸€å€‹æ–°çš„å”ç¨‹ç‰©ä»¶ã€‚

        Returns:
            æˆåŠŸåŸ·è¡Œå¾Œçš„çµæœã€‚
        
        Raises:
            Exception: å¦‚æœæ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œå‰‡æ‹‹å‡ºæœ€å¾Œä¸€æ¬¡çš„ç•°å¸¸ã€‚
        """
        last_exception = None
        for attempt in range(self.max_retries + 1):
            try:
                task_coro = coro_factory()
                result = await asyncio.wait_for(task_coro, timeout=self.diagnosis_timeout)
                if attempt > 0:
                    logger.info(f"âœ… å·¥å…· {name} åœ¨ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¾ŒæˆåŠŸã€‚")
                return result
            except Exception as e:
                last_exception = e
                if attempt < self.max_retries:
                    # æŒ‡æ•¸é€€é¿å»¶é²
                    delay = self.retry_delay * (2 ** attempt)
                    logger.warning(
                        f"âš ï¸ å·¥å…· {name} ç¬¬ {attempt + 1}/{self.max_retries + 1} æ¬¡åŸ·è¡Œå¤±æ•—: {e}. "
                        f"å°‡åœ¨ {delay:.2f} ç§’å¾Œé‡è©¦..."
                    )
                    await asyncio.sleep(delay)
                else:
                    logger.error(
                        f"âŒ å·¥å…· {name} åœ¨ {self.max_retries + 1} æ¬¡å˜—è©¦å¾Œæœ€çµ‚å¤±æ•—ã€‚"
                    )
                    raise last_exception

    async def _execute_parallel_tasks(self, tool_tasks: List[tuple]) -> Dict[str, ToolResult]:
        """
        ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹ç•°æ­¥è¨ºæ–·å·¥å…·ä»»å‹™ï¼Œæ¯å€‹ä»»å‹™éƒ½åŒ…å«é‡è©¦é‚è¼¯ã€‚

        ä½¿ç”¨ `asyncio.gather` ä¾†ä¸¦ç™¼åŸ·è¡Œæ‰€æœ‰å·¥å…·ï¼Œä¸¦é€é `return_exceptions=True`
        ç¢ºä¿å³ä½¿æŸå€‹å·¥å…·æœ€çµ‚å¤±æ•—ï¼Œå…¶ä»–å·¥å…·ä¹Ÿèƒ½ç¹¼çºŒåŸ·è¡Œã€‚

        Args:
            tool_tasks: ä¸€å€‹åŒ…å« (åç¨±, å”ç¨‹å·¥å» ) çš„å…ƒçµ„åˆ—è¡¨ã€‚

        Returns:
            ä¸€å€‹å­—å…¸ï¼Œéµæ˜¯å·¥å…·åç¨±ï¼Œå€¼æ˜¯å…¶ `ToolResult`ã€‚
        """
        results = {}

        tasks_to_gather = [
            self._run_task_with_retry(name, coro_factory) for name, coro_factory in tool_tasks
        ]

        task_results = await asyncio.gather(*tasks_to_gather, return_exceptions=True)
        
        for i, (name, _) in enumerate(tool_tasks):
            result = task_results[i]
            if isinstance(result, Exception):
                # The exception is already logged in _run_task_with_retry
                # We just need to format it for the final result.
                error_code = "TOOL_TIMEOUT_ERROR" if isinstance(result, asyncio.TimeoutError) else "TOOL_EXECUTION_ERROR"
                results[name] = ToolResult(success=False, error=ToolError(code=error_code, message=str(result)))
            else:
                results[name] = result
        return results
    
    def _analyze_deployment_results(self, results: Dict[str, ToolResult], request: DiagnosticRequest) -> DiagnosticResult:
        """åˆ†æéƒ¨ç½²è¨ºæ–·çµæœ"""
        all_findings = []
        tools_used = []
        
        if "prometheus" in results and results["prometheus"].success:
            tools_used.append("PrometheusQueryTool")
            metrics = results["prometheus"].data
            if float(metrics.get("cpu_usage", "0%").replace("%", "")) > 80:
                all_findings.append(Finding(source="Prometheus", severity="critical", message="CPU ä½¿ç”¨ç‡éé«˜", evidence=metrics))
            if float(metrics.get("memory_usage", "0%").replace("%", "")) > 90:
                all_findings.append(Finding(source="Prometheus", severity="critical", message="è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜", evidence=metrics))

        if "loki" in results and results["loki"].success:
            tools_used.append("LokiLogQueryTool")
            logs = results["loki"].data
            # ä¿®æ­£ï¼šå°é½Šæ¸¬è©¦ä¸­æ¨¡æ“¬çš„ã€æ›´çœŸå¯¦çš„å·¢ç‹€è³‡æ–™çµæ§‹
            analysis = logs.get("analysis", {})
            critical_indicators = analysis.get("critical_indicators")
            if critical_indicators:
                all_findings.append(Finding(source="Loki", severity="critical", message=f"ç™¼ç¾åš´é‡æ—¥èªŒæŒ‡æ¨™: {', '.join(critical_indicators)}", evidence=logs))

        if "audit" in results and results["audit"].success:
            tools_used.append("ControlPlaneTool (Audit)")
            audit = results["audit"].data
            if audit.get("logs"):
                all_findings.append(Finding(source="Control-Plane", severity="info", message=f"ç™¼ç¾ {len(audit['logs'])} ç­†ç›¸é—œå¯©è¨ˆæ—¥èªŒã€‚", evidence=audit))

        if "incidents" in results and results["incidents"].success:
            tools_used.append("ControlPlaneTool (Incidents)")
            incidents = results["incidents"].data
            if incidents.get("incidents"):
                all_findings.append(Finding(source="Control-Plane", severity="warning", message=f"ç™¼ç¾ {len(incidents['incidents'])} ä»¶ç›¸é—œçš„æ´»èºäº‹ä»¶ã€‚", evidence=incidents))

        if all_findings:
            summary = f"è¨ºæ–·å®Œæˆï¼Œå…±ç™¼ç¾ {len(all_findings)} å€‹å•é¡Œé»ã€‚"
            recommended_actions = ["è«‹æ ¹æ“šç™¼ç¾çš„è©³ç´°è³‡è¨Šé€²è¡Œæ·±å…¥èª¿æŸ¥ã€‚"]
            confidence_score = 0.8
        else:
            summary = "åˆæ­¥è¨ºæ–·æœªç™¼ç¾æ˜é¡¯å•é¡Œã€‚"
            recommended_actions = ["å»ºè­°æ‰‹å‹•æª¢æŸ¥æœå‹™æ—¥èªŒå’Œç›£æ§å„€è¡¨æ¿ã€‚"]
            confidence_score = 0.5
            
        return DiagnosticResult(
            summary=summary,
            findings=all_findings,
            recommended_actions=recommended_actions,
            confidence_score=confidence_score,
            tools_used=tools_used
        )

    async def analyze_capacity(self, request: CapacityAnalysisRequest) -> CapacityAnalysisResponse:
        """
        åˆ†æçµ¦å®šè³‡æºçš„å®¹é‡ã€‚

        æ­¤æ–¹æ³•æœƒä¸¦è¡ŒæŸ¥è©¢æ‰€æœ‰è«‹æ±‚è³‡æºçš„é£½å’Œåº¦æŒ‡æ¨™ï¼Œ
        è¨ˆç®—å¹³å‡ä½¿ç”¨ç‡ï¼Œä¸¦ç”¢ç”Ÿä¸€å€‹ç°¡å–®çš„é æ¸¬å’Œå»ºè­°ã€‚
        """
        logger.info(f"ğŸ“ˆ é–‹å§‹åˆ†æå®¹é‡ï¼Œè³‡æº: {request.resource_ids}")

        # ç‚ºæ¯å€‹è³‡æºå»ºç«‹ä¸€å€‹ä¸¦è¡ŒæŸ¥è©¢ä»»å‹™
        tasks = [
            self._run_task_with_retry(
                name=f"saturation_check_{resource_id}",
                coro_factory=functools.partial(
                    self.prometheus_tool.execute,
                    {"service": resource_id, "metric_type": "saturation"}
                )
            )
            for resource_id in request.resource_ids
        ]

        # åŸ·è¡Œæ‰€æœ‰æŸ¥è©¢
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # è™•ç†å’ŒåŒ¯ç¸½çµæœ
        total_cpu = 0
        total_mem = 0
        valid_results = 0

        for result in results:
            if isinstance(result, ToolResult) and result.success and result.data:
                saturation_data = result.data.get("saturation", {})
                try:
                    cpu_usage = float(saturation_data.get("cpu_usage", "0%").replace("%", ""))
                    mem_usage = float(saturation_data.get("memory_usage", "0%").replace("%", ""))
                    total_cpu += cpu_usage
                    total_mem += mem_usage
                    valid_results += 1
                except (ValueError, TypeError):
                    logger.warning(f"ç„¡æ³•è§£æé£½å’Œåº¦æ•¸æ“š: {saturation_data}")
                    continue

        if valid_results == 0:
            logger.warning("æ‰€æœ‰è³‡æºçš„å®¹é‡æŒ‡æ¨™æŸ¥è©¢å‡å¤±æ•—ã€‚")
            # è¿”å›ä¸€å€‹è¡¨ç¤ºå¤±æ•—æˆ–æ•¸æ“šä¸è¶³çš„å›æ‡‰
            return CapacityAnalysisResponse(
                current_usage={"average": 0, "peak": 0},
                forecast={"trend": "unknown", "days_to_capacity": -1},
                recommendations=[{"type": "investigate", "resource": "all", "priority": "high", "reasoning": "ç„¡æ³•ç²å–ä»»ä½•è³‡æºçš„å®¹é‡æŒ‡æ¨™ã€‚"}]
            )

        avg_cpu = total_cpu / valid_results
        avg_mem = total_mem / valid_results

        # ç”¢ç”Ÿç°¡å–®çš„é æ¸¬å’Œå»ºè­°
        trend = "stable"
        days_to_capacity = 999
        recommendations = []

        if avg_cpu > 85 or avg_mem > 85:
            trend = "critical"
            days_to_capacity = 1
            recommendations.append({
                "type": "scale_up",
                "resource": "all_requested",
                "priority": "critical",
                "reasoning": f"å¹³å‡è³‡æºä½¿ç”¨ç‡éé«˜ (CPU: {avg_cpu:.1f}%, Memory: {avg_mem:.1f}%)"
            })
        elif avg_cpu > 70 or avg_mem > 70:
            trend = "increasing"
            days_to_capacity = 30
            recommendations.append({
                "type": "scale_up",
                "resource": "all_requested",
                "priority": "high",
                "reasoning": f"å¹³å‡è³‡æºä½¿ç”¨ç‡åé«˜ (CPU: {avg_cpu:.1f}%, Memory: {avg_mem:.1f}%)"
            })
        else:
            recommendations.append({
                "type": "none",
                "resource": "all_requested",
                "priority": "low",
                "reasoning": "ç›®å‰è³‡æºä½¿ç”¨ç‡åœ¨æ­£å¸¸ç¯„åœå…§ã€‚"
            })

        return CapacityAnalysisResponse(
            current_usage={"average": round((avg_cpu + avg_mem) / 2, 2), "peak": max(avg_cpu, avg_mem)},
            forecast={"trend": trend, "days_to_capacity": days_to_capacity},
            recommendations=recommendations
        )

    async def _diagnose_alerts(self, session_id: uuid.UUID, request: AlertAnalysisRequest, status: DiagnosticStatus) -> DiagnosticResult:
        """
        è¨ºæ–·å‘Šè­¦å•é¡Œã€‚

        æ­¤æ–¹æ³•æœƒä¸¦è¡Œèª¿æŸ¥å¤šå€‹å‘Šè­¦ï¼Œæ”¶é›†ç›¸é—œçš„æ—¥èªŒå’ŒæŒ‡æ¨™ï¼Œ
        ä¸¦å°‡çµæœåŒ¯ç¸½æˆä¸€ä»½è¨ºæ–·å ±å‘Šã€‚
        """
        logger.info(f"ğŸ” [Session: {session_id}] é–‹å§‹è¨ºæ–·å‘Šè­¦: {request.alert_ids}")
        status.current_step = "å»ºç«‹ä¸¦è¡Œå‘Šè­¦èª¿æŸ¥ä»»å‹™"
        status.progress = 20
        await self._update_task_status(session_id, status)

        # ç‚ºæ¯å€‹å‘Šè­¦ ID å»ºç«‹ä¸€å€‹èª¿æŸ¥ä»»å‹™
        investigation_tasks = [
            self._investigate_single_alert(alert_id, request.correlation_window)
            for alert_id in request.alert_ids
        ]

        # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰èª¿æŸ¥
        all_findings_lists = await asyncio.gather(*investigation_tasks, return_exceptions=True)

        status.current_step = "æ•´åˆèˆ‡åˆ†æèª¿æŸ¥çµæœ"
        status.progress = 80
        await self._update_task_status(session_id, status)

        # è™•ç†ä¸¦æ‰å¹³åŒ–çµæœ
        all_findings: List[Finding] = []
        tools_used = set()
        for result in all_findings_lists:
            if isinstance(result, Exception):
                logger.error(f"èª¿æŸ¥ä»»å‹™å¤±æ•—: {result}", exc_info=True)
                all_findings.append(Finding(
                    source="SREWorkflow",
                    severity="warning",
                    message=f"ä¸€å€‹å‘Šè­¦èª¿æŸ¥å­ä»»å‹™å¤±æ•—: {result}"
                ))
            elif isinstance(result, tuple):
                findings, used_tool_names = result
                all_findings.extend(findings)
                tools_used.update(used_tool_names)

        # ç”¢ç”Ÿæœ€çµ‚å ±å‘Š
        if not all_findings:
            summary = "å®Œæˆå‘Šè­¦åˆ†æï¼Œæœªç™¼ç¾ä»»ä½•å…·é«”çš„æ—¥èªŒæˆ–æŒ‡æ¨™ç•°å¸¸ã€‚"
            recommended_actions = ["å»ºè­°æ‰‹å‹•æª¢æŸ¥å‘Šè­¦å„€è¡¨æ¿ä»¥ç²å–æ›´å¤šä¸Šä¸‹æ–‡ã€‚"]
            confidence_score = 0.4
        else:
            summary = f"å‘Šè­¦åˆ†æå®Œæˆï¼Œå…±ç”¢ç”Ÿ {len(all_findings)} æ¢ç›¸é—œç™¼ç¾ã€‚"
            # TODO: Add more sophisticated summary logic
            recommended_actions = ["è«‹æª¢é–±ä¸‹æ–¹çš„ç™¼ç¾ä»¥äº†è§£è©³ç´°è³‡è¨Šã€‚", "æ ¹æ“šç™¼ç¾çš„åš´é‡æ€§æ¡å–è¡Œå‹•ã€‚"]
            confidence_score = 0.75

        return DiagnosticResult(
            summary=summary,
            findings=all_findings,
            recommended_actions=recommended_actions,
            confidence_score=confidence_score,
            tools_used=list(tools_used)
        )

    async def _investigate_single_alert(self, alert_id: str, window_seconds: int) -> tuple[List[Finding], List[str]]:
        """
        èª¿æŸ¥å–®ä¸€å‘Šè­¦ï¼Œç²å–ç›¸é—œçš„æ—¥èªŒå’ŒæŒ‡æ¨™ã€‚
        """
        findings: List[Finding] = []
        tools_used: List[str] = []

        # 1. å¾ Control Plane ç²å–å‘Šè­¦è©³æƒ…
        incident_result = await self.control_plane_tool.query_incidents(params={"search": alert_id, "limit": 1})
        tools_used.append("ControlPlaneTool (Incidents)")

        if not incident_result.success or not incident_result.data.get("incidents"):
            findings.append(Finding(source="ControlPlaneTool", severity="warning", message=f"ç„¡æ³•ç²å– ID ç‚º {alert_id} çš„å‘Šè­¦è©³æƒ…ã€‚"))
            return findings, tools_used

        incident = incident_result.data["incidents"][0]
        service_name = incident.get("service_name", "unknown-service")
        alert_time_str = incident.get("created_at", datetime.now(timezone.utc).isoformat())
        alert_time = datetime.fromisoformat(alert_time_str)

        findings.append(Finding(
            source="ControlPlaneTool",
            severity="info",
            message=f"æ­£åœ¨èª¿æŸ¥å‘Šè­¦ '{incident.get('title', alert_id)}'ï¼Œå½±éŸ¿æœå‹™: {service_name}ã€‚",
            evidence=incident,
            timestamp=alert_time
        ))

        # 2. æº–å‚™ä¸¦è¡ŒæŸ¥è©¢æ—¥èªŒå’ŒæŒ‡æ¨™
        half_window = window_seconds / 2
        start_time = alert_time - timedelta(seconds=half_window)
        end_time = alert_time + timedelta(seconds=half_window)

        loki_params = {
            "service": service_name,
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "limit": 100
        }

        # å‡è¨­å‘Šè­¦è¦å‰‡ä¸­å¯èƒ½åŒ…å« prometheus æŸ¥è©¢
        # é€™éƒ¨åˆ†é‚è¼¯å¯ä»¥æ ¹æ“šå¯¦éš›çš„ `incident` è³‡æ–™çµæ§‹é€²è¡Œæ“´å……
        prom_query = incident.get("details", {}).get("prometheus_query")

        tool_tasks = [
            ("loki", functools.partial(self.loki_tool.execute, loki_params))
        ]
        if prom_query:
            prom_params = {"query": prom_query, "time": alert_time.isoformat()}
            tool_tasks.append(("prometheus", functools.partial(self.prometheus_tool.execute, prom_params)))

        # 3. åŸ·è¡ŒæŸ¥è©¢
        results = await self._execute_parallel_tasks(tool_tasks)

        # 4. è™•ç†æŸ¥è©¢çµæœ
        if "loki" in results:
            tools_used.append("LokiLogQueryTool")
            if results["loki"].success and results["loki"].data.get("logs"):
                log_count = len(results["loki"].data["logs"])
                findings.append(Finding(
                    source="Loki",
                    severity="info",
                    message=f"åœ¨å‘Šè­¦æ™‚é–“çª—å£å…§ç™¼ç¾ {log_count} æ¢æ—¥èªŒã€‚",
                    evidence=results["loki"].data
                ))

        if "prometheus" in results:
            tools_used.append("PrometheusQueryTool")
            if results["prometheus"].success and results["prometheus"].data.get("value"):
                findings.append(Finding(
                    source="Prometheus",
                    severity="info",
                    message="åŸ·è¡Œå‘Šè­¦ç›¸é—œçš„ Prometheus æŸ¥è©¢æˆåŠŸã€‚",
                    evidence=results["prometheus"].data
                ))

        return findings, tools_used

    async def _analyze_capacity(self, session_id: uuid.UUID, request: CapacityAnalysisRequest, status: DiagnosticStatus) -> CapacityAnalysisResponse:
        """
        åˆ†æå®¹é‡å•é¡Œ (éª¨æ¶)
        """
        logger.info(f"ğŸ“ˆ [Session: {session_id}] åˆ†æå®¹é‡: {request.resource_ids}")
        return CapacityAnalysisResponse(
            current_usage={"average": 55.5, "peak": 80.2},
            forecast={"trend": "increasing", "days_to_capacity": 45},
            recommendations=[{"type": "scale_up", "resource": request.resource_ids[0], "priority": "high", "reasoning": "é æ¸¬ä½¿ç”¨é‡å°‡åœ¨ 45 å¤©å¾Œé”åˆ°ç“¶é ¸"}]
        )

    async def _execute_query(self, session_id: uuid.UUID, request: ExecuteRequest, status: DiagnosticStatus) -> DiagnosticResult:
        """
        åŸ·è¡Œè‡ªç„¶èªè¨€æŸ¥è©¢ (éª¨æ¶)
        """
        logger.info(f"ğŸ¤– [Session: {session_id}] åŸ·è¡ŒæŸ¥è©¢: {request.query}")
        status.current_step = "è§£æè‡ªç„¶èªè¨€æŸ¥è©¢"
        status.progress = 30
        await self._update_task_status(session_id, status)
        await asyncio.sleep(1)

        status.current_step = "åŸ·è¡Œå°æ‡‰çš„å·¥å…·"
        status.progress = 70
        await self._update_task_status(session_id, status)
        await asyncio.sleep(2)
        return DiagnosticResult(summary=f"å·²åŸ·è¡ŒæŸ¥è©¢: '{request.query}'", findings=[], recommended_actions=["ç„¡"])