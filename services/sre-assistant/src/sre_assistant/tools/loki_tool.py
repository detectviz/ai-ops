# services/sre-assistant/src/sre_assistant/tools/loki_tool.py
"""
Loki æ—¥èªŒæŸ¥è©¢å·¥å…·
ç”¨æ–¼æŸ¥è©¢å’Œåˆ†ææœå‹™æ—¥èªŒ
"""

import logging
import httpx
import json
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta, timezone
from urllib.parse import quote

from ..contracts import ToolResult, ToolError

logger = logging.getLogger(__name__)


class LokiLogQueryTool:
    """
    Loki æ—¥èªŒæŸ¥è©¢å·¥å…·
    
    åŠŸèƒ½ï¼š
    - æŸ¥è©¢éŒ¯èª¤æ—¥èªŒ
    - åˆ†ææ—¥èªŒæ¨¡å¼
    - çµ±è¨ˆæ—¥èªŒç´šåˆ¥åˆ†ä½ˆ
    - æå–é—œéµéŒ¯èª¤è¨Šæ¯
    """
    
    def __init__(self, config):
        """åˆå§‹åŒ– Loki å·¥å…·"""
        self.base_url = config.loki.base_url
        self.timeout = config.loki.timeout_seconds
        self.default_limit = config.loki.default_limit
        self.max_time_range = config.loki.max_time_range
        
        logger.info(f"âœ… Loki å·¥å…·åˆå§‹åŒ–: {self.base_url}")
    
    async def execute(self, params: Dict[str, Any]) -> ToolResult:
        """
        åŸ·è¡Œ Loki æ—¥èªŒæŸ¥è©¢
        
        Args:
            params: æŸ¥è©¢åƒæ•¸
                - service: æœå‹™åç¨±
                - namespace: å‘½åç©ºé–“
                - log_level: æ—¥èªŒç´šåˆ¥ (error/warn/info/debug)
                - pattern: æœå°‹æ¨¡å¼
                - time_range: æ™‚é–“ç¯„åœï¼ˆåˆ†é˜ï¼‰
                - limit: è¿”å›ç­†æ•¸é™åˆ¶
                
        Returns:
            ToolResult åŒ…å«æŸ¥è©¢çµæœ
        """
        try:
            service = params.get("service", "")
            namespace = params.get("namespace", "default")
            log_level = params.get("log_level", "error")
            pattern = params.get("pattern", "")
            time_range = params.get("time_range", 30)  # é è¨­ 30 åˆ†é˜
            limit = params.get("limit", self.default_limit)
            
            logger.info(f"ğŸ“ æŸ¥è©¢ Loki: service={service}, level={log_level}, pattern={pattern}")
            
            # åŸ·è¡ŒæŸ¥è©¢
            logs = await self._query_logs(
                service=service,
                namespace=namespace,
                log_level=log_level,
                pattern=pattern,
                time_range=time_range,
                limit=limit
            )
            
            # åˆ†ææ—¥èªŒ
            analysis = self._analyze_logs(logs)
            
            return ToolResult(
                success=True,
                data={
                    "logs": logs,
                    "analysis": analysis,
                    "query_params": {
                        "service": service,
                        "namespace": namespace,
                        "log_level": log_level,
                        "time_range": f"{time_range}m"
                    }
                },
                metadata={
                    "source": "loki",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "total_logs": len(logs)
                }
            )
            
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ Loki API æŸ¥è©¢å¤±æ•—: {e.response.status_code} - {e.response.text}", exc_info=True)
            return ToolResult(
                success=False,
                error=ToolError(
                    code="HTTP_STATUS_ERROR",
                    message=f"Loki API returned HTTP {e.response.status_code}: {e.response.reason_phrase}",
                    details={
                        "status_code": e.response.status_code,
                        "response_body": e.response.text[:500],  # é™åˆ¶å›æ‡‰å…§å®¹é•·åº¦
                        "request_url": str(e.request.url),
                        "params": params
                    }
                )
            )
        except httpx.TimeoutException as e:
            logger.error(f"âŒ Loki API è«‹æ±‚è¶…æ™‚: {e}", exc_info=True)
            return ToolResult(
                success=False,
                error=ToolError(
                    code="TIMEOUT_ERROR",
                    message=f"Loki API request timed out after {self.timeout}s",
                    details={
                        "timeout_seconds": self.timeout,
                        "request_url": str(e.request.url) if hasattr(e, 'request') else None,
                        "params": params
                    }
                )
            )
        except httpx.ConnectError as e:
            logger.error(f"âŒ Loki API é€£ç·šå¤±æ•—: {e}", exc_info=True)
            return ToolResult(
                success=False,
                error=ToolError(
                    code="CONNECTION_ERROR",
                    message=f"Failed to connect to Loki: {str(e)}",
                    details={
                        "base_url": self.base_url,
                        "params": params
                    }
                )
            )
        except Exception as e:
            logger.error(f"âŒ Loki å·¥å…·åŸ·è¡Œæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=True)
            return ToolResult(
                success=False,
                error=ToolError(
                    code="UNEXPECTED_ERROR",
                    message=f"Unexpected error in Loki tool: {str(e)}",
                    details={
                        "error_type": type(e).__name__,
                        "params": params
                    }
                )
            )
    
    async def query_logs_by_service(
        self,
        service_name: str,
        namespace: str,
        level: str,
        duration: str
    ) -> Dict[str, Any]:
        """
        æ ¹æ“šæœå‹™æŸ¥è©¢å’Œåˆ†ææ—¥èªŒ
        
        Args:
            service_name: æœå‹™åç¨±
            namespace: å‘½åç©ºé–“
            level: æ—¥èªŒç´šåˆ¥
            duration: æ™‚é–“ç¯„åœ (e.g., "5m", "1h")
            
        Returns:
            æ—¥èªŒåˆ†æçµæœ
        """
        # è§£ææ™‚é–“ç¯„åœ
        import re
        time_value = int(re.search(r'\d+', duration).group())
        time_unit = re.search(r'[a-zA-Z]+', duration).group()
        
        if time_unit == "m":
            time_range_minutes = time_value
        elif time_unit == "h":
            time_range_minutes = time_value * 60
        else:
            time_range_minutes = 5 # é è¨­

        logs = await self._query_logs(
            service=service_name,
            namespace=namespace,
            log_level=level,
            pattern="", # æš«ä¸ä½¿ç”¨
            time_range=time_range_minutes,
            limit=self.default_limit
        )
        
        return self._analyze_logs(logs)

    async def _query_logs(
        self,
        service: str,
        namespace: str,
        log_level: str,
        pattern: str,
        time_range: int,
        limit: int
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥è©¢æ—¥èªŒ
        
        Returns:
            æ—¥èªŒåˆ—è¡¨
        """
        # å»ºæ§‹ LogQL æŸ¥è©¢
        query = self._build_logql_query(service, namespace, log_level, pattern)
        
        # è¨ˆç®—æ™‚é–“ç¯„åœ
        end_time = datetime.now(timezone.utc)
        start_time = end_time - timedelta(minutes=time_range)
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            params = {
                "query": query,
                "start": str(int(start_time.timestamp() * 1e9)),  # å¥ˆç§’
                "end": str(int(end_time.timestamp() * 1e9)),
                "limit": limit,
                "direction": "backward"  # æœ€æ–°çš„æ—¥èªŒå„ªå…ˆ
            }
            
            response = await client.get(
                f"{self.base_url}/loki/api/v1/query_range",
                params=params
            )
            
            # è®“ä¸Šå±¤çš„ execute æ–¹æ³•ä¾†è™•ç† HTTP éŒ¯èª¤
            response.raise_for_status()
            
            data = response.json()
            
            if data.get("status") != "success":
                # Loki æŸ¥è©¢æœ¬èº«å¯èƒ½å¤±æ•— (ä¾‹å¦‚èªæ³•éŒ¯èª¤)
                error_msg = data.get('error', 'Unknown Loki query error')
                logger.warning(f"Loki æŸ¥è©¢æˆåŠŸä½†èªæ³•æˆ–åŸ·è¡Œå¤±æ•—: {error_msg}")
                # é€™ç¨®æƒ…æ³ä¸‹ä¹Ÿè¿”å›ç©ºåˆ—è¡¨ï¼Œå› ç‚ºå®ƒæ˜¯ä¸€å€‹æœ‰æ•ˆçš„â€œç„¡çµæœâ€å ´æ™¯
                return []
            
            # è§£ææ—¥èªŒ
            return self._parse_log_results(data.get("data", {}).get("result", []))
    
    def _build_logql_query(self, service: str, namespace: str, log_level: str, pattern: str) -> str:
        """
        å»ºæ§‹ LogQL æŸ¥è©¢èªå¥
        
        Returns:
            LogQL æŸ¥è©¢å­—ä¸²
        """
        # åŸºæœ¬æ¨™ç±¤é¸æ“‡å™¨
        selectors = []
        
        if service:
            selectors.append(f'app="{service}"')
        if namespace:
            selectors.append(f'namespace="{namespace}"')
        
        # å¦‚æœæ²’æœ‰ä»»ä½•é¸æ“‡å™¨ï¼Œä½¿ç”¨é è¨­
        if not selectors:
            selectors.append('job=~".+"')
        
        query = "{" + ",".join(selectors) + "}"
        
        # åŠ å…¥æ—¥èªŒç´šåˆ¥éæ¿¾
        if log_level and log_level.lower() != "all":
            level_patterns = {
                "error": "(?i)(error|err|fatal|panic|critical)",
                "warn": "(?i)(warn|warning)",
                "info": "(?i)(info|information)",
                "debug": "(?i)(debug|trace)"
            }
            if log_level.lower() in level_patterns:
                query += f' |~ "{level_patterns[log_level.lower()]}"'
        
        # åŠ å…¥è‡ªå®šç¾©æ¨¡å¼
        if pattern:
            query += f' |~ "{pattern}"'
        
        return query
    
    def _parse_log_results(self, results: List[Dict]) -> List[Dict[str, Any]]:
        """
        è§£æ Loki æŸ¥è©¢çµæœ
        
        Returns:
            è§£æå¾Œçš„æ—¥èªŒåˆ—è¡¨
        """
        logs = []
        
        for stream in results:
            stream_labels = stream.get("stream", {})
            values = stream.get("values", [])
            
            for value in values:
                if len(value) >= 2:
                    timestamp_ns = int(value[0])
                    log_line = value[1]
                    
                    # è§£ææ—¥èªŒå…§å®¹
                    parsed_log = self._parse_log_line(log_line)
                    
                    logs.append({
                        "timestamp": datetime.fromtimestamp(timestamp_ns / 1e9, tz=timezone.utc).isoformat(),
                        "labels": stream_labels,
                        "message": log_line,
                        "parsed": parsed_log
                    })
        
        # æŒ‰æ™‚é–“æ’åºï¼ˆæœ€æ–°å„ªå…ˆï¼‰
        logs.sort(key=lambda x: x["timestamp"], reverse=True)
        
        return logs
    
    def _parse_log_line(self, log_line: str) -> Dict[str, Any]:
        """
        è§£æå–®è¡Œæ—¥èªŒ
        
        å˜—è©¦è§£æ JSON æ ¼å¼ï¼Œå¦å‰‡è¿”å›ç´”æ–‡å­—
        """
        # å˜—è©¦è§£æ JSON
        try:
            return json.loads(log_line)
        except json.JSONDecodeError:
            pass
        
        # å˜—è©¦è§£æå¸¸è¦‹çš„æ—¥èªŒæ ¼å¼
        parsed = {
            "raw": log_line,
            "level": self._extract_log_level(log_line),
            "error_type": self._extract_error_type(log_line)
        }
        
        return parsed
    
    def _extract_log_level(self, log_line: str) -> str:
        """æå–æ—¥èªŒç´šåˆ¥"""
        log_line_lower = log_line.lower()
        
        if any(word in log_line_lower for word in ["error", "err", "fatal", "panic", "critical"]):
            return "ERROR"
        elif any(word in log_line_lower for word in ["warn", "warning"]):
            return "WARN"
        elif any(word in log_line_lower for word in ["info", "information"]):
            return "INFO"
        elif any(word in log_line_lower for word in ["debug", "trace"]):
            return "DEBUG"
        
        return "UNKNOWN"
    
    def _extract_error_type(self, log_line: str) -> Optional[str]:
        """æå–éŒ¯èª¤é¡å‹"""
        # å¸¸è¦‹éŒ¯èª¤æ¨¡å¼
        error_patterns = {
            "OOMKilled": "è¨˜æ†¶é«”ä¸è¶³",
            "Connection refused": "é€£æ¥è¢«æ‹’çµ•",
            "Connection timeout": "é€£æ¥è¶…æ™‚",
            "NullPointerException": "ç©ºæŒ‡æ¨™ç•°å¸¸",
            "StackOverflowError": "å †ç–Šæº¢å‡º",
            "OutOfMemoryError": "è¨˜æ†¶é«”ä¸è¶³",
            "DeadlockDetected": "æ­»é–åµæ¸¬",
            "Circuit breaker": "ç†”æ–·å™¨è§¸ç™¼",
            "Rate limit": "é€Ÿç‡é™åˆ¶",
            "401": "æœªæˆæ¬Š",
            "403": "ç¦æ­¢è¨ªå•",
            "404": "è³‡æºæœªæ‰¾åˆ°",
            "500": "å…§éƒ¨ä¼ºæœå™¨éŒ¯èª¤",
            "502": "é–˜é“éŒ¯èª¤",
            "503": "æœå‹™ä¸å¯ç”¨",
            "504": "é–˜é“è¶…æ™‚"
        }
        
        for pattern, error_type in error_patterns.items():
            if pattern in log_line:
                return error_type
        
        return None
    
    def _analyze_logs(self, logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆ†ææ—¥èªŒæ¨¡å¼å’Œçµ±è¨ˆ
        
        Returns:
            åˆ†æçµæœ
        """
        if not logs:
            return {
                "total_logs": 0,
                "level_distribution": {},
                "error_types": {},
                "top_errors": []
            }
        
        # çµ±è¨ˆæ—¥èªŒç´šåˆ¥åˆ†ä½ˆ
        level_counts = {}
        error_types = {}
        error_messages = []
        
        for log in logs:
            # çµ±è¨ˆç´šåˆ¥
            level = log.get("parsed", {}).get("level", "UNKNOWN")
            level_counts[level] = level_counts.get(level, 0) + 1
            
            # çµ±è¨ˆéŒ¯èª¤é¡å‹
            error_type = log.get("parsed", {}).get("error_type")
            if error_type:
                error_types[error_type] = error_types.get(error_type, 0) + 1
            
            # æ”¶é›†éŒ¯èª¤è¨Šæ¯
            if level == "ERROR":
                error_messages.append(log.get("message", ""))
        
        # æ‰¾å‡ºæœ€å¸¸è¦‹çš„éŒ¯èª¤
        top_errors = []
        if error_messages:
            # ç°¡å–®çš„éŒ¯èª¤èšé¡ï¼ˆåŸºæ–¼ç›¸ä¼¼åº¦ï¼‰
            error_clusters = self._cluster_errors(error_messages)
            top_errors = sorted(error_clusters.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return {
            "total_logs": len(logs),
            "level_distribution": level_counts,
            "error_types": error_types,
            "top_errors": [{"pattern": pattern, "count": count} for pattern, count in top_errors],
            "critical_indicators": self._identify_critical_indicators(logs)
        }
    
    def _cluster_errors(self, error_messages: List[str]) -> Dict[str, int]:
        """
        ç°¡å–®çš„éŒ¯èª¤èšé¡
        
        åŸºæ–¼é—œéµè©æå–ç›¸ä¼¼éŒ¯èª¤
        """
        clusters = {}
        
        for message in error_messages:
            # æå–é—œéµè©ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
            key_parts = []
            
            # å°‹æ‰¾ç•°å¸¸é¡å‹
            if "Exception" in message:
                exception_start = message.find("Exception")
                exception_end = message.find(" ", exception_start)
                if exception_end == -1:
                    exception_end = len(message)
                key_parts.append(message[exception_start:exception_end])
            
            # å°‹æ‰¾éŒ¯èª¤ä»£ç¢¼
            import re
            error_codes = re.findall(r'\b[4-5]\d{2}\b', message)
            if error_codes:
                key_parts.extend(error_codes)
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç‰¹å®šæ¨¡å¼ï¼Œä½¿ç”¨å‰ 50 å€‹å­—å…ƒ
            if not key_parts:
                key_parts.append(message[:50])
            
            cluster_key = " | ".join(key_parts)
            clusters[cluster_key] = clusters.get(cluster_key, 0) + 1
        
        return clusters
    
    def _identify_critical_indicators(self, logs: List[Dict[str, Any]]) -> List[str]:
        """
        è­˜åˆ¥é—œéµæŒ‡æ¨™
        
        Returns:
            é—œéµå•é¡ŒæŒ‡æ¨™åˆ—è¡¨
        """
        indicators = []
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ OOM
        oom_count = sum(1 for log in logs if "OOMKilled" in log.get("message", ""))
        if oom_count > 0:
            indicators.append(f"ç™¼ç¾ {oom_count} æ¬¡è¨˜æ†¶é«”ä¸è¶³éŒ¯èª¤ (OOMKilled)")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ Panic
        panic_count = sum(1 for log in logs if "panic" in log.get("message", "").lower())
        if panic_count > 0:
            indicators.append(f"ç™¼ç¾ {panic_count} æ¬¡ Panic éŒ¯èª¤")
        
        # æª¢æŸ¥é€£æ¥éŒ¯èª¤
        conn_errors = sum(1 for log in logs if any(
            pattern in log.get("message", "")
            for pattern in ["Connection refused", "Connection timeout", "connection reset"]
        ))
        if conn_errors > 5:
            indicators.append(f"ç™¼ç¾ {conn_errors} æ¬¡é€£æ¥éŒ¯èª¤ï¼Œå¯èƒ½å­˜åœ¨ç¶²è·¯å•é¡Œ")
        
        # æª¢æŸ¥éŒ¯èª¤ç‡
        error_logs = sum(1 for log in logs if log.get("parsed", {}).get("level") == "ERROR")
        if len(logs) > 0:
            error_rate = (error_logs / len(logs)) * 100
            if error_rate > 50:
                indicators.append(f"éŒ¯èª¤ç‡éé«˜: {error_rate:.1f}%")
        
        return indicators

    # --- Roadmap Task 1.3: Log Aggregation ---

    async def _execute_aggregation_query(self, query: str, time_range: int) -> int:
        """
        åŸ·è¡Œ Loki èšåˆæŸ¥è©¢ (å¦‚ count_over_time)ã€‚

        Args:
            query: å®Œæ•´çš„ LogQL èšåˆæŸ¥è©¢èªå¥ã€‚
            time_range: æŸ¥è©¢çš„æ™‚é–“ç¯„åœï¼ˆåˆ†é˜ï¼‰ã€‚

        Returns:
            èšåˆå¾Œçš„è¨ˆæ•¸ï¼Œå¤±æ•—å‰‡è¿”å› 0ã€‚
        """
        end_time = datetime.now(timezone.utc)
        start_time = end_time - timedelta(minutes=time_range)

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                params = {
                    "query": query,
                    "start": str(int(start_time.timestamp() * 1e9)),
                    "end": str(int(end_time.timestamp() * 1e9)),
                    "step": f"{time_range}m" # ä½¿ç”¨æ•´å€‹ç¯„åœä½œç‚ºå–®ä¸€æ­¥é•·
                }
                response = await client.get(f"{self.base_url}/loki/api/v1/query", params=params)
                response.raise_for_status()
                data = response.json()

                if data.get("status") != "success":
                    logger.warning(f"Loki èšåˆæŸ¥è©¢å¤±æ•—: {data.get('error', 'Unknown error')}")
                    return 0

                result = data.get("data", {}).get("result", [])
                if result and result[0] and len(result[0].get("value", [])) > 1:
                    return int(result[0]["value"][1])
                return 0
        except httpx.HTTPStatusError as e:
            logger.error(f"åŸ·è¡ŒèšåˆæŸ¥è©¢æ™‚ç™¼ç”Ÿ HTTP éŒ¯èª¤: {e.response.status_code}")
            return 0
        except Exception as e:
            logger.error(f"åŸ·è¡ŒèšåˆæŸ¥è©¢æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}", exc_info=True)
            return 0

    async def aggregate_logs_by_level(
        self, service: str, namespace: str, time_range: int
    ) -> Dict[str, int]:
        """
        ä½¿ç”¨ LogQL åœ¨ä¼ºæœå™¨ç«¯æŒ‰æ—¥èªŒç´šåˆ¥èšåˆæ—¥èªŒæ•¸é‡ã€‚

        é€™æ˜¯ä¸€å€‹æ¢ç´¢æ€§åŠŸèƒ½ï¼Œç”¨æ–¼èˆ‡å®¢æˆ¶ç«¯åˆ†æé€²è¡Œæ¯”è¼ƒã€‚
        å®ƒé€éç‚ºæ¯å€‹ç´šåˆ¥åŸ·è¡Œä¸¦è¡Œçš„ count_over_time æŸ¥è©¢ä¾†å·¥ä½œã€‚
        """
        logger.info(f"ğŸ§ª åŸ·è¡Œ Loki ä¼ºæœå™¨ç«¯æ—¥èªŒèšåˆ: service={service}, time_range={time_range}m")

        base_selector = "{" + f'app="{service}",namespace="{namespace}"' + "}"
        time_filter = f"[{time_range}m]"

        level_patterns = {
            "error": '(?i)(error|err|fatal|panic|critical)',
            "warn": '(?i)(warn|warning)',
            "info": '(?i)(info|information)',
        }

        tasks = {}
        for level, pattern in level_patterns.items():
            # LogQL for count_over_time with a filter
            query = f"count_over_time({base_selector} |~ `{pattern}` {time_filter})"
            tasks[level] = self._execute_aggregation_query(query, time_range)

        # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰èšåˆæŸ¥è©¢
        results = await asyncio.gather(*tasks.values())

        # å°‡çµæœèˆ‡ç´šåˆ¥åç¨±å°æ‡‰èµ·ä¾†
        level_counts = {level: count for level, count in zip(tasks.keys(), results)}

        logger.info(f"ğŸ“Š Loki ä¼ºæœå™¨ç«¯èšåˆçµæœ: {level_counts}")
        return level_counts