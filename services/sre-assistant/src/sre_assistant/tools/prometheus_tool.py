# services/sre-assistant/src/sre_assistant/tools/prometheus_tool.py
"""
Prometheus æŸ¥è©¢å·¥å…·
ç”¨æ–¼æŸ¥è©¢æœå‹™çš„é—œéµæŒ‡æ¨™ï¼ˆå››å¤§é»ƒé‡‘è¨Šè™Ÿï¼‰
"""

import structlog
import httpx
import json
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta, timezone
from urllib.parse import urlencode

from ..contracts import ToolResult, ToolError

logger = structlog.get_logger(__name__)


class PrometheusQueryTool:
    """
    Prometheus æŸ¥è©¢å·¥å…·
    
    å¯¦ä½œ SRE å››å¤§é»ƒé‡‘è¨Šè™ŸæŸ¥è©¢ï¼š
    - Latency (å»¶é²)
    - Traffic (æµé‡)
    - Errors (éŒ¯èª¤)
    - Saturation (é£½å’Œåº¦)
    """
    
    def __init__(self, config, redis_client=None):
        """
        åˆå§‹åŒ– Prometheus å·¥å…·

        Args:
            config: æ‡‰ç”¨ç¨‹å¼è¨­å®šç‰©ä»¶ã€‚
            redis_client: éåŒæ­¥ Redis å®¢æˆ¶ç«¯ï¼Œç”¨æ–¼å¿«å–ã€‚
        """
        self.base_url = config.prometheus.base_url
        self.timeout = config.prometheus.timeout_seconds
        self.default_step = config.prometheus.default_step
        self.max_points = config.prometheus.max_points
        
        # å¿«å–è¨­å®š
        self.redis_client = redis_client
        self.cache_ttl_seconds = config.prometheus.get("cache_ttl_seconds", 300) # é è¨­ 5 åˆ†é˜

        if self.redis_client:
            logger.info(f"âœ… Prometheus å·¥å…·åˆå§‹åŒ– (å« Redis å¿«å–): {self.base_url}")
        else:
            logger.info(f"âœ… Prometheus å·¥å…·åˆå§‹åŒ– (ç„¡å¿«å–): {self.base_url}")
    
    async def execute(self, params: Dict[str, Any]) -> ToolResult:
        """
        åŸ·è¡Œ Prometheus æŸ¥è©¢
        
        Args:
            params: åŒ…å«æŸ¥è©¢åƒæ•¸çš„å­—å…¸
                - service: æœå‹™åç¨±
                - namespace: å‘½åç©ºé–“
                - metric_type: æŒ‡æ¨™é¡å‹ (latency/traffic/errors/saturation)
                - time_range: æ™‚é–“ç¯„åœï¼ˆåˆ†é˜ï¼‰
                
        Returns:
            ToolResult åŒ…å«æŸ¥è©¢çµæœæˆ–éŒ¯èª¤
        """
        try:
            service = params.get("service", "")
            namespace = params.get("namespace", "default")
            metric_type = params.get("metric_type", "all")
            time_range = params.get("time_range", 30)  # é è¨­ 30 åˆ†é˜
            
            logger.info(f"ğŸ“Š æŸ¥è©¢ Prometheus: service={service}, namespace={namespace}, type={metric_type}")
            
            # å„ªå…ˆè™•ç†è‡ªå®šç¾©æŸ¥è©¢
            query = params.get("query")
            if query:
                metrics = await self._query_custom(query, time_range)
            # å¦å‰‡ï¼Œæ ¹æ“šæŒ‡æ¨™é¡å‹åŸ·è¡ŒæŸ¥è©¢
            elif metric_type == "all":
                metrics = await self.query_golden_signals(service, namespace, time_range)
            elif metric_type == "latency":
                metrics = await self._query_latency(service, namespace, time_range)
            elif metric_type == "traffic":
                metrics = await self._query_traffic(service, namespace, time_range)
            elif metric_type == "errors":
                metrics = await self._query_errors(service, namespace, time_range)
            elif metric_type == "saturation":
                metrics = await self._query_saturation(service, namespace, time_range)
            else:
                # ä¿ç•™ä¸€å€‹å¾Œå‚™ï¼Œå„˜ç®¡åœ¨ä¸Šé¢çš„é‚è¼¯ä¸­ä¸å¤ªå¯èƒ½åˆ°é”
                metrics = {"error": f"æœªçŸ¥æŒ‡æ¨™é¡å‹: {metric_type}"}
            
            return ToolResult(
                success=True,
                data=metrics,
                metadata={
                    "source": "prometheus",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "query_time_range": f"{time_range}m"
                }
            )
            
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ Prometheus API æŸ¥è©¢å¤±æ•—: {e.response.status_code} - {e.response.text}", exc_info=True)
            return ToolResult(
                success=False,
                error=ToolError(
                    code="HTTP_STATUS_ERROR",
                    message=f"Prometheus API returned HTTP {e.response.status_code}: {e.response.reason_phrase}",
                    details={
                        "status_code": e.response.status_code,
                        "response_body": e.response.text[:500],  # é™åˆ¶å›æ‡‰å…§å®¹é•·åº¦
                        "request_url": str(e.request.url),
                        "params": params
                    }
                )
            )
        except httpx.TimeoutException as e:
            logger.error(f"âŒ Prometheus API è«‹æ±‚è¶…æ™‚: {e}", exc_info=True)
            return ToolResult(
                success=False,
                error=ToolError(
                    code="TIMEOUT_ERROR",
                    message=f"Prometheus API request timed out after {self.timeout}s",
                    details={
                        "timeout_seconds": self.timeout,
                        "request_url": str(e.request.url) if hasattr(e, 'request') else None,
                        "params": params
                    }
                )
            )
        except httpx.ConnectError as e:
            logger.error(f"âŒ Prometheus API é€£ç·šå¤±æ•—: {e}", exc_info=True)
            return ToolResult(
                success=False,
                error=ToolError(
                    code="CONNECTION_ERROR",
                    message=f"Failed to connect to Prometheus: {str(e)}",
                    details={
                        "base_url": self.base_url,
                        "params": params
                    }
                )
            )
        except Exception as e:
            logger.error(f"âŒ Prometheus å·¥å…·åŸ·è¡Œæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}", exc_info=True)
            return ToolResult(
                success=False,
                error=ToolError(
                    code="UNEXPECTED_ERROR",
                    message=f"Unexpected error in Prometheus tool: {str(e)}",
                    details={
                        "error_type": type(e).__name__,
                        "params": params
                    }
                )
            )
    
    async def query_golden_signals(self, service_name: str, namespace: str, duration: str) -> Dict[str, Any]:
        """æŸ¥è©¢å››å¤§é»ƒé‡‘è¨Šè™Ÿ"""
        results = {}
        
        # ä¸¦è¡ŒæŸ¥è©¢æ‰€æœ‰æŒ‡æ¨™
        import asyncio
        tasks = [
            self._query_latency(service_name, namespace, duration),
            self._query_traffic(service_name, namespace, duration),
            self._query_errors(service_name, namespace, duration),
            self._query_saturation(service_name, namespace, duration)
        ]
        
        latency, traffic, errors, saturation = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è™•ç†çµæœ
        if not isinstance(latency, Exception):
            results["latency"] = latency
        if not isinstance(traffic, Exception):
            results["traffic"] = traffic
        if not isinstance(errors, Exception):
            results["errors"] = errors
        if not isinstance(saturation, Exception):
            results["saturation"] = saturation
        
        return results
    
    async def _query_latency(self, service: str, namespace: str, time_range: int) -> Dict[str, Any]:
        """æŸ¥è©¢å»¶é²æŒ‡æ¨™"""
        # P50, P95, P99 å»¶é²
        queries = {
            "p50": f'histogram_quantile(0.50, rate(http_request_duration_seconds_bucket{{service="{service}", namespace="{namespace}"}}[5m]))',
            "p95": f'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{{service="{service}", namespace="{namespace}"}}[5m]))',
            "p99": f'histogram_quantile(0.99, rate(http_request_duration_seconds_bucket{{service="{service}", namespace="{namespace}"}}[5m]))'
        }
        
        results = {}
        for percentile, query in queries.items():
            value = await self._execute_instant_query(query)
            if value is not None:
                results[percentile] = f"{value*1000:.2f}ms"
        
        return results
    
    async def _query_traffic(self, service: str, namespace: str, time_range: int) -> Dict[str, Any]:
        """æŸ¥è©¢æµé‡æŒ‡æ¨™"""
        # è«‹æ±‚ç‡ (RPS)
        query = f'sum(rate(http_requests_total{{service="{service}", namespace="{namespace}"}}[5m]))'
        
        rps = await self._execute_instant_query(query)
        
        return {
            "requests_per_second": round(rps, 2) if rps else 0,
            "requests_per_minute": round(rps * 60, 2) if rps else 0
        }
    
    async def _query_errors(self, service: str, namespace: str, time_range: int) -> Dict[str, Any]:
        """æŸ¥è©¢éŒ¯èª¤æŒ‡æ¨™"""
        # éŒ¯èª¤ç‡
        error_query = f'sum(rate(http_requests_total{{service="{service}", namespace="{namespace}", status=~"5.."}}[5m]))'
        total_query = f'sum(rate(http_requests_total{{service="{service}", namespace="{namespace}"}}[5m]))'
        
        errors = await self._execute_instant_query(error_query)
        total = await self._execute_instant_query(total_query)
        
        error_rate = 0
        if total and total > 0:
            error_rate = (errors / total) * 100
        
        return {
            "error_rate": f"{error_rate:.2f}%",
            "errors_per_minute": round(errors * 60, 2) if errors else 0
        }
    
    async def _query_saturation(self, service: str, namespace: str, time_range: int) -> Dict[str, Any]:
        """æŸ¥è©¢é£½å’Œåº¦æŒ‡æ¨™"""
        queries = {
            "cpu_usage": f'avg(rate(container_cpu_usage_seconds_total{{pod=~"{service}.*", namespace="{namespace}"}}[5m])) * 100',
            "memory_usage": f'avg(container_memory_usage_bytes{{pod=~"{service}.*", namespace="{namespace}"}}) / avg(container_spec_memory_limit_bytes{{pod=~"{service}.*", namespace="{namespace}"}}) * 100',
            "disk_usage": f'avg(container_fs_usage_bytes{{pod=~"{service}.*", namespace="{namespace}"}}) / avg(container_fs_limit_bytes{{pod=~"{service}.*", namespace="{namespace}"}}) * 100'
        }
        
        results = {}
        for metric, query in queries.items():
            value = await self._execute_instant_query(query)
            if value is not None:
                results[metric] = f"{value:.2f}%"
        
        # æŸ¥è©¢ Pod æ•¸é‡
        pod_query = f'count(up{{job="{service}", namespace="{namespace}"}})'
        pod_count = await self._execute_instant_query(pod_query)
        results["pod_count"] = int(pod_count) if pod_count else 0
        
        return results
    
    async def _query_custom(self, query: str, time_range: int) -> Dict[str, Any]:
        """åŸ·è¡Œè‡ªå®šç¾©æŸ¥è©¢"""
        if not query:
            return {"error": "No query provided"}
        
        value = await self._execute_instant_query(query)
        return {"value": value, "query": query}
    
    async def _execute_instant_query(self, query: str) -> Optional[float]:
        """
        åŸ·è¡Œå³æ™‚æŸ¥è©¢ï¼Œä¸¦å¢åŠ  Redis å¿«å–æ©Ÿåˆ¶ã€‚
        
        Args:
            query: PromQL æŸ¥è©¢èªå¥
            
        Returns:
            æŸ¥è©¢çµæœçš„æ•¸å€¼ï¼Œå¦‚æœç„¡çµæœå‰‡è¿”å› None
        """
        cache_key = f"prometheus:instant:{query}"

        # 1. æª¢æŸ¥å¿«å–
        if self.redis_client:
            try:
                cached_result = await self.redis_client.get(cache_key)
                if cached_result:
                    logger.info(f"CACHE HIT: å¾ Redis ç²å–å³æ™‚æŸ¥è©¢çµæœ: {query}")
                    # Redis å„²å­˜çš„æ˜¯ JSON å­—ä¸²ï¼Œéœ€è¦è§£æ
                    data = json.loads(cached_result)
                    return float(data) if data is not None else None
            except Exception as e:
                logger.error(f"Redis å¿«å–è®€å–å¤±æ•—: {e}")

        # 2. å¿«å–æœªå‘½ä¸­ï¼ŒåŸ·è¡Œå¯¦éš›æŸ¥è©¢
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            params = {"query": query, "time": datetime.now(timezone.utc).isoformat()}
            response = await client.get(f"{self.base_url}/api/v1/query", params=params)
            response.raise_for_status() # å¦‚æœç‹€æ…‹ç¢¼ä¸æ˜¯ 2xxï¼Œå‰‡æ‹‹å‡º HTTPStatusError
            
            data = response.json()
            
            if data["status"] != "success":
                # å¦‚æœ Prometheus å›æ‡‰æˆåŠŸä½†æŸ¥è©¢æœ¬èº«å¤±æ•—ï¼Œè¨˜éŒ„éŒ¯èª¤ä½†ç¹¼çºŒï¼ˆé¿å…ä½¿æ•´å€‹å·¥å…·å¤±æ•—ï¼‰
                logger.warning(f"Prometheus æŸ¥è©¢ '{query}' æˆåŠŸåŸ·è¡Œä½†æœªè¿”å› 'success' ç‹€æ…‹: {data.get('error', 'Unknown error')}")
                return None
            
            value_to_cache = None
            results = data.get("data", {}).get("result", [])
            if results and len(results) > 0:
                value = results[0].get("value", [])
                if len(value) > 1:
                    value_to_cache = float(value[1])

            # 3. å„²å­˜çµæœåˆ°å¿«å–
            if self.redis_client and value_to_cache is not None:
                try:
                    await self.redis_client.set(
                        cache_key,
                        json.dumps(value_to_cache),
                        ex=self.cache_ttl_seconds,
                    )
                    logger.info(f"CACHE SET: å·²å¿«å–å³æ™‚æŸ¥è©¢çµæœ: {query}")
                except Exception as e:
                    logger.error(f"Redis å¿«å–å¯«å…¥å¤±æ•—: {e}") # å¿«å–å¤±æ•—ä¸æ‡‰ä¸­æ–·ä¸»æµç¨‹
            
            return value_to_cache
    
    async def _execute_range_query(self, query: str, start: datetime, end: datetime, step: str = "1m") -> List[Dict]:
        """
        åŸ·è¡Œç¯„åœæŸ¥è©¢ï¼Œä¸¦å¢åŠ  Redis å¿«å–æ©Ÿåˆ¶ã€‚
        
        Args:
            query: PromQL æŸ¥è©¢èªå¥
            start: é–‹å§‹æ™‚é–“
            end: çµæŸæ™‚é–“
            step: æ­¥é•·
            
        Returns:
            æ™‚é–“åºåˆ—æ•¸æ“šåˆ—è¡¨
        """
        # ç‚ºå¿«å–éµæ¨™æº–åŒ–æ™‚é–“ï¼Œé¿å…å› å¾®ç§’å·®ç•°å°è‡´å¿«å–å¤±æ•ˆ
        start_key = start.strftime('%Y-%m-%dT%H:%M')
        end_key = end.strftime('%Y-%m-%dT%H:%M')
        cache_key = f"prometheus:range:{query}:{start_key}:{end_key}:{step}"

        # 1. æª¢æŸ¥å¿«å–
        if self.redis_client:
            try:
                cached_result = await self.redis_client.get(cache_key)
                if cached_result:
                    logger.info(f"CACHE HIT: å¾ Redis ç²å–ç¯„åœæŸ¥è©¢çµæœ: {query}")
                    return json.loads(cached_result)
            except Exception as e:
                logger.error(f"Redis å¿«å–è®€å–å¤±æ•—: {e}")

        # 2. å¿«å–æœªå‘½ä¸­ï¼ŒåŸ·è¡Œå¯¦éš›æŸ¥è©¢
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            params = {
                "query": query,
                "start": start.isoformat(),
                "end": end.isoformat(),
                "step": step
            }
            response = await client.get(f"{self.base_url}/api/v1/query_range", params=params)
            response.raise_for_status() # å¦‚æœç‹€æ…‹ç¢¼ä¸æ˜¯ 2xxï¼Œå‰‡æ‹‹å‡º HTTPStatusError
            
            data = response.json()
            
            if data["status"] != "success":
                logger.warning(f"Prometheus æŸ¥è©¢ '{query}' æˆåŠŸåŸ·è¡Œä½†æœªè¿”å› 'success' ç‹€æ…‹: {data.get('error', 'Unknown error')}")
                return []
            
            result_to_cache = data.get("data", {}).get("result", [])

            # 3. å„²å­˜çµæœåˆ°å¿«å–
            if self.redis_client and result_to_cache:
                try:
                    await self.redis_client.set(
                        cache_key,
                        json.dumps(result_to_cache),
                        ex=self.cache_ttl_seconds,
                    )
                    logger.info(f"CACHE SET: å·²å¿«å–ç¯„åœæŸ¥è©¢çµæœ: {query}")
                except Exception as e:
                    logger.error(f"Redis å¿«å–å¯«å…¥å¤±æ•—: {e}") # å¿«å–å¤±æ•—ä¸æ‡‰ä¸­æ–·ä¸»æµç¨‹

            return result_to_cache